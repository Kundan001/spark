{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import col,lit\n",
    "from pyspark.sql import functions as F,Window\n",
    "from pyspark.sql.types import BooleanType, IntegerType, StringType\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from functools import reduce\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesRegressor, ExtraTreesClassifier\n",
    "from sklearn.svm import LinearSVC, LinearSVR\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "\n",
    "from statsmodels.tools.tools import add_constant\n",
    "from sklearn.metrics import classification_report, cohen_kappa_score\n",
    "import scipy.stats as stats\n",
    "\n",
    "from joblib import dump, load\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "import traceback\n",
    "\n",
    "from pandas.api.types import is_string_dtype, is_numeric_dtype\n",
    "\n",
    "from typing import Any\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "so_code = \"BC\"\n",
    "current_part = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Lap-114:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1cc5f036eb8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate() \n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visit_plan_path = \"data\\\\\" + so_code + \"\\\\visit_plan_BC.csv\"\n",
    "# shipment_data_path = \"data\\\\\" + so_code + \"\\\\shipments_BC.csv\"\n",
    "# date_analysis_data_path = \"data\\\\\" + so_code + \"\\\\bank_holidays_BC.csv\"\n",
    "# shipment_split_data_path = \"data\\\\\" + so_code + \"\\\\shipment_split_vp_BC.csv\"\n",
    "# sr_loading_data_path = \"data\\\\\" + so_code + \"\\\\sr_loading_BC.csv\"\n",
    "# sr_unloading_data_path = \"data\\\\\" + so_code + \"\\\\sr_unloading_BC.csv\"\n",
    "# stock_collection_data_path = \"data\\\\\" + so_code + \"\\\\stock_collection_BC.csv\"\n",
    "# credit_requests_data_path = \"data\\\\\" + so_code + \"\\\\credit_requests_BC.csv\"\n",
    "# pre_easter_effect_data_path = \"data\\\\\" + so_code + \"\\\\pre_easter_week_data.csv\"\n",
    "# test_period_start_date = pd.Timestamp(year=2018, month=1, day=1)\n",
    "\n",
    "root_path = \"D:\\\\Projects\\\\JTI\\\\Romania\\\\data\\\\\"\n",
    "\n",
    "visit_plan_path = root_path + so_code + \"\\\\pickles\\\\visit_plan_BC.pkl\"\n",
    "shipment_data_path = root_path + so_code + \"\\\\pickles\\\\shipments_BC.pkl\"\n",
    "date_analysis_data_path = root_path + so_code + \"\\\\pickles\\\\bank_holidays_BC.pkl\"\n",
    "shipment_split_data_path = root_path + so_code + \"\\\\pickles\\\\shipment_split_vp_BC.pkl\"\n",
    "sr_loading_data_path = root_path + so_code + \"\\\\pickles\\\\sr_loading_BC.pkl\"\n",
    "sr_unloading_data_path = root_path + so_code + \"\\\\pickles\\\\sr_unloading_BC.pkl\"\n",
    "stock_collection_data_path = root_path + so_code + \"\\\\pickles\\\\stock_collection_BC.pkl\"\n",
    "credit_requests_data_path = root_path + so_code + \"\\\\pickles\\\\credit_requests_BC.pkl\"\n",
    "pre_easter_effect_data_path = root_path + so_code + \"\\\\pickles\\\\pre_easter_week_data.pkl\"\n",
    "test_period_start_date = pd.Timestamp(year=2018, month=1, day=1)\n",
    "\n",
    "pred_start_date = test_period_start_date\n",
    "y_col = 'shipments'\n",
    "so_code = \"BC\"\n",
    "output_path = \"D:\\\\Romania\\\\\"\n",
    "run_log_path = \"D:\\\\Romania\\\\\"\n",
    "\n",
    "run_log_path = os.path.join(output_path,\n",
    "                                'run_log_{}_{}.txt'.format(so_code,\n",
    "                                                           current_part))\n",
    "\n",
    "remove_first_month_from_training = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "take_shipment_data_cols = ['invoice_date', 'agent_code', 'product_code',\n",
    "                           'quantity', 'pos_code', 'promo_id', 'return']\n",
    "\n",
    "take_future_visit_plan_data_cols = ['visit_date', 'agent_code',\n",
    "                                    'pos_code', 'visit_order_day']\n",
    "\n",
    "take_pos_visit_plan_data_cols = ['visit_date', 'pos_code',\n",
    "                                 'pre_nonreplacement_holiday',\n",
    "                                 'double_sell', 'triple_sell']\n",
    "\n",
    "take_holidays_data_cols = ['DayNumberOfWeek', 'VisitPlanWeek', 'IsWorkingDate',\n",
    "                           'NationalDoubleSell', 'NationalTripleSell',\n",
    "                           'PricelistChgAnn', 'PricelistChg']\n",
    "\n",
    "take_credit_req_data_cols = ['PlannedStartDate', 'PlannedClosedDate',\n",
    "                             'ReqType', 'IncrCoef', 'AgentCode']\n",
    "\n",
    "preprocess_constant_cols = ['product_code', 'pos_code', 'product_cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     33
    ]
   },
   "outputs": [],
   "source": [
    "# Reconcile agent replacements, holiday adjustments in visit plan data\n",
    "def __visit_plan_reconciliation(visit_plan_data: pd.DataFrame,\n",
    "                                weekday_holiday_dates: list) -> pd.DataFrame:\n",
    "    visit_plan_df: pd.DataFrame = visit_plan_data.reset_index()\n",
    "    visit_plan_df = visit_plan_df.sort_values(['visit_date'])\n",
    "    visit_plan_df.loc[\n",
    "        visit_plan_df['visit_date'].isin(weekday_holiday_dates),\n",
    "        'weekday_holiday'] = True\n",
    "    visit_plan_df['weekday_holiday'] = visit_plan_df['weekday_holiday'].fillna(False)\n",
    "\n",
    "    def __create_new_markers(pos_plan_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        current_pos = pos_plan_df['pos_code'].iloc[0]\n",
    "        # print(current_pos)\n",
    "        pos_plan_df['type_lead_1'] = pos_plan_df['type'].shift(-1)\n",
    "        pos_plan_df.loc[\n",
    "            pos_plan_df['type_lead_1'] == 'Holiday without replacement',\n",
    "            'pre_nonreplacement_holiday'] = True\n",
    "        pos_plan_df['pre_nonreplacement_holiday'] = \\\n",
    "            pos_plan_df['pre_nonreplacement_holiday'].fillna(False)\n",
    "        pos_plan_df['double_sell'] = pos_plan_df['weekday_holiday'].shift(-1).fillna(False)\n",
    "        pos_plan_df['weekday_holiday(-2)'] = pos_plan_df['weekday_holiday'].shift(-2).fillna(False)\n",
    "        pos_plan_df['triple_sell'] = (pos_plan_df['double_sell']\n",
    "                                      & pos_plan_df['weekday_holiday(-2)'])\n",
    "        pos_plan_df.loc[pos_plan_df['triple_sell'], 'double_sell'] = False\n",
    "        return pos_plan_df.drop(columns=['type_lead_1', 'weekday_holiday(-2)'])\n",
    "\n",
    "    visit_plan_df = visit_plan_df.groupby(['pos_code']).apply(\n",
    "        __create_new_markers)\n",
    "    # Holiday without replacement means no agent actually went there\n",
    "    visit_plan_df = visit_plan_df.loc[\n",
    "        visit_plan_df['type'] != 'Holiday without replacement']\n",
    "    # Take only non-weekday-holiday dates\n",
    "    visit_plan_df = visit_plan_df.loc[~visit_plan_df['weekday_holiday']]\n",
    "    visit_plan_df.loc[visit_plan_df['type'] == 'Replacement', 'agent_code'] = \\\n",
    "        visit_plan_df.loc[visit_plan_df['type'] == 'Replacement',\n",
    "                          'backupsalesagent_code'].astype(int)\n",
    "\n",
    "    return visit_plan_df\n",
    "\n",
    "\n",
    "# Add derived columns as required to date_analysis_data\n",
    "def __date_analysis_additions(date_analysis_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    # manual correction according to seen data\n",
    "    date_analysis_data.loc[\n",
    "        [pd.Timestamp(day=7, month=12, year=2018)],\n",
    "        'IsWorkingDate'] = 0\n",
    "    date_analysis_data['WeekdayHoliday'] = ((date_analysis_data['DayNumberOfWeek'] <= 5)\n",
    "                                            & (date_analysis_data['IsWorkingDate'] == 0))\n",
    "    date_analysis_data['PriceChgPeriod'] = (date_analysis_data['PricelistChgAnn']\n",
    "                                            | date_analysis_data['PricelistChg'])\n",
    "    date_analysis_data['PriceChgPeriod'] = date_analysis_data['PriceChgPeriod'].cumsum()\n",
    "    date_analysis_data['PriceChgEffect'] = \\\n",
    "        date_analysis_data['PriceChgPeriod'].apply(lambda x: x % 2 != 0)\n",
    "    date_analysis_data['PriceChgPeriod'] = (date_analysis_data['PriceChgEffect']\n",
    "                                            | date_analysis_data['PricelistChg'])\n",
    "    # Convert to sparse then query index to find block locations\n",
    "    temp_ts: pd.SparseSeries = date_analysis_data['PriceChgPeriod'].to_sparse(\n",
    "        fill_value=False)\n",
    "    block_locs = zip(temp_ts.sp_index.blocs, temp_ts.sp_index.blengths)\n",
    "    # Map the sparse blocks back to the dense timeseries\n",
    "    block_infos = [(date_analysis_data['PriceChgPeriod'].iloc[start:(start + length)],\n",
    "                    length)\n",
    "                   for (start, length) in block_locs]\n",
    "    for series_block, length in block_infos:\n",
    "        values = range(length)\n",
    "        date_analysis_data.loc[series_block.index, 'daysSincePriceChgAnn'] = values\n",
    "        date_analysis_data.loc[series_block.index, 'daysFromPriceChg'] = values[::-1]\n",
    "    date_analysis_data['daysSincePriceChgAnn'] = \\\n",
    "        date_analysis_data['daysSincePriceChgAnn'].fillna(-1).astype(int)\n",
    "    date_analysis_data['daysFromPriceChg'] = \\\n",
    "        date_analysis_data['daysFromPriceChg'].fillna(-1).astype(int)\n",
    "\n",
    "    return date_analysis_data.drop(columns=['PricelistChgAnn',\n",
    "                                            'PriceChgPeriod',\n",
    "                                            'PriceChgEffect'])\n",
    "\n",
    "\n",
    "# prepare shipment data into consumable format\n",
    "def __prep_consumable_shipments_data(shipment_data: pd.DataFrame,\n",
    "                                     train_period_shipments: pd.DataFrame) -> pd.DataFrame:\n",
    "    shipment_records: pd.DataFrame = shipment_data.loc[shipment_data['return'] == 0].copy()\n",
    "    shipments_train_agg: pd.DataFrame = train_period_shipments.groupby(\n",
    "        ['invoice_date', 'product_code']).agg({'quantity': 'sum'})\n",
    "\n",
    "    shipments_train_agg = shipments_train_agg.reset_index().groupby(\n",
    "        ['product_code']).agg({'quantity': 'mean'})\n",
    "    shipments_train_agg['contribution%'] = (shipments_train_agg['quantity']\n",
    "                                            / shipments_train_agg['quantity'].sum())*100\n",
    "    shipments_train_agg = shipments_train_agg.sort_values(['contribution%'])\n",
    "    shipments_train_agg['contribution%_cumsum'] = shipments_train_agg['contribution%'].cumsum()\n",
    "    # take products which make up 5% or a little more than 5% of the total quantity\n",
    "    num_small_products = len(shipments_train_agg.loc[\n",
    "                                 shipments_train_agg['contribution%_cumsum'] < 5])\n",
    "    small_products = shipments_train_agg.index[:num_small_products+1]\n",
    "    # pd.Series(small_products).to_csv('small_products_BW.csv')\n",
    "    num_medium_products = len(shipments_train_agg.loc[\n",
    "                                  (shipments_train_agg['contribution%_cumsum'] >= 5)\n",
    "                                  & (shipments_train_agg['contribution%_cumsum'] < 20)])\n",
    "    medium_products = shipments_train_agg.index[num_small_products+1\n",
    "                                                : num_small_products+num_medium_products+1]\n",
    "    # pd.Series(medium_products).to_csv('medium_products_BW.csv')\n",
    "\n",
    "    shipment_records = shipment_records.groupby(['invoice_date', 'pos_code',\n",
    "                                                 'product_code']).agg({'agent_code': 'first',\n",
    "                                                                       'quantity': 'sum',\n",
    "                                                                       'promo_id': 'first'})\n",
    "\n",
    "    return_records: pd.DataFrame = shipment_data.loc[shipment_data['return'] == 1].copy()\n",
    "    return_records = return_records.groupby(['invoice_date', 'pos_code',\n",
    "                                             'product_code']).agg({'agent_code': 'first',\n",
    "                                                                   'quantity': 'sum',\n",
    "                                                                   'return': 'first'})\n",
    "    return_records['return'] = return_records['return'].astype('bool')\n",
    "\n",
    "    shipment_records['return'] = return_records['return']\n",
    "    shipment_records['return'] = shipment_records['return'].fillna(False)\n",
    "    shipment_records = shipment_records.reset_index()\n",
    "    shipment_records.loc[\n",
    "        shipment_records['product_code'].isin(small_products), 'product_cat'] = 'small'\n",
    "    shipment_records['product_cat'] = shipment_records['product_cat'].fillna('large')\n",
    "    shipment_records = shipment_records.set_index('invoice_date')\n",
    "    return shipment_records.sort_index()\n",
    "\n",
    "\n",
    "def __prep_credit_req_data(credit_req_data: pd.DataFrame,\n",
    "                           pred_start_date):\n",
    "    credit_req_components: list = []\n",
    "\n",
    "    def __flatten_credit_req_days(row: pd.DataFrame) -> None:\n",
    "        start_date = row[\"PlannedStartDate\"]\n",
    "        end_date = row[\"PlannedClosedDate\"]\n",
    "\n",
    "        ret = pd.DataFrame(index=pd.date_range(start_date, end_date))\n",
    "        ret.loc[:, 'request_type'] = row['ReqType']\n",
    "        ret.loc[:, 'increment_coeff'] = row['IncrCoef']\n",
    "        ret.loc[:, 'agent_code'] = row['AgentCode']\n",
    "\n",
    "        credit_req_components.append(ret)\n",
    "\n",
    "    credit_req_data.apply(__flatten_credit_req_days, axis=1)\n",
    "    processed_credit_req_data = pd.concat(credit_req_components)\n",
    "    processed_credit_req_data['agent_code'] = processed_credit_req_data[\n",
    "        'agent_code'].astype('int').astype('str')\n",
    "    processed_credit_req_data.index.name = 'visit_date'\n",
    "    processed_credit_req_data = processed_credit_req_data.reset_index()\n",
    "    processed_credit_req_data = processed_credit_req_data.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    return processed_credit_req_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(visit_plan_path: str,\n",
    "              shipment_data_path: str,\n",
    "              date_analysis_data_path: str,\n",
    "              shipment_split_data_path: str,\n",
    "              sr_loading_data_path: str,\n",
    "              sr_unloading_data_path: str,\n",
    "              stock_collection_data_path: str,\n",
    "              credit_requests_data_path: str,\n",
    "              pre_easter_effect_data_path: str,\n",
    "              pred_start_date: pd.Timestamp) -> tuple:\n",
    "    shipment_data: pd.DataFrame = pd.read_pickle(shipment_data_path)\n",
    "    shipment_data = shipment_data[take_shipment_data_cols]\n",
    "    shipment_data['agent_code'] = shipment_data['agent_code'].astype(str)\n",
    "    shipment_data['pos_code'] = shipment_data['pos_code'].astype(str)\n",
    "    products_to_consider = shipment_data.loc[\n",
    "        shipment_data['invoice_date']\n",
    "        >= pd.Timestamp(day=1, month=1, year=2018)]['product_code'].unique()\n",
    "    shipment_data = shipment_data.loc[\n",
    "        shipment_data['product_code'].isin(products_to_consider)]\n",
    "    shipment_records = __prep_consumable_shipments_data(shipment_data,\n",
    "                                                        shipment_data[\n",
    "                                                            (shipment_data['invoice_date']\n",
    "                                                             < pd.Timestamp(day=1,\n",
    "                                                                            month=1,\n",
    "                                                                            year=2018))\n",
    "                                                        ])\n",
    "    print('Completed loading shipment data.')\n",
    "\n",
    "    date_analysis_data: pd.DataFrame = pd.read_pickle(date_analysis_data_path)\n",
    "    date_analysis_data = date_analysis_data.set_index('Date')\n",
    "    date_analysis_data = date_analysis_data[take_holidays_data_cols]\n",
    "    date_analysis_data = __date_analysis_additions(date_analysis_data)\n",
    "    print('Completed loading different date features data.')\n",
    "\n",
    "    visit_plan_data: pd.DataFrame = pd.read_pickle(visit_plan_path)\n",
    "    weekday_holiday_dates = date_analysis_data[date_analysis_data['WeekdayHoliday']].index.tolist()\n",
    "    visit_plan_data = __visit_plan_reconciliation(visit_plan_data, weekday_holiday_dates)\n",
    "    visit_plan_data['agent_code'] = visit_plan_data['agent_code'].astype(str)\n",
    "    visit_plan_data['pos_code'] = visit_plan_data['pos_code'].astype(str)\n",
    "    # remove any weekend dates, if any, from visit plan\n",
    "    visit_plan_data = visit_plan_data[\n",
    "        visit_plan_data['visit_date'].isin(\n",
    "            date_analysis_data[date_analysis_data['IsWorkingDate'] == 1].index)]\n",
    "\n",
    "    future_visit_plan_data = visit_plan_data.loc[\n",
    "        visit_plan_data['visit_date'] >= pred_start_date,\n",
    "        take_future_visit_plan_data_cols].copy()\n",
    "    future_visit_plan_data.set_index(['visit_date', 'agent_code', 'pos_code'],\n",
    "                                     inplace=True)\n",
    "    future_visit_plan_data.sort_index(inplace=True)\n",
    "\n",
    "    pos_visit_plan_data = visit_plan_data\n",
    "    pos_visit_plan_data = pos_visit_plan_data[take_pos_visit_plan_data_cols]\n",
    "    pos_visit_plan_data.set_index(['pos_code'], inplace=True)\n",
    "    print('Completed loading visit plan related data.')\n",
    "\n",
    "    shipment_split_data: pd.DataFrame = pd.read_pickle(shipment_split_data_path)\n",
    "    shipment_split_data['agent_code'] = shipment_split_data['agent_code'].astype(str)\n",
    "    shipment_split_data.set_index(['visit_date', 'agent_code', 'product_code'],\n",
    "                                  inplace=True)\n",
    "    print('Completed loading ER, non-ER shipment split data.')\n",
    "\n",
    "    sr_loading_data: pd.DataFrame = pd.read_pickle(sr_loading_data_path)\n",
    "    sr_loading_data['agent_code'] = sr_loading_data['agent_code'].astype(str)\n",
    "    sr_loading_data.set_index(['visit_date', 'agent_code', 'product_code'],\n",
    "                              inplace=True)\n",
    "    print('Completed loading SR loading data.')\n",
    "\n",
    "    sr_unloading_data: pd.DataFrame = pd.read_pickle(sr_unloading_data_path)\n",
    "    sr_unloading_data['agent_code'] = sr_unloading_data['agent_code'].astype(str)\n",
    "    sr_unloading_data.set_index(['visit_date', 'agent_code', 'product_code'],\n",
    "                                inplace=True)\n",
    "    print('Completed loading SR unloading data.')\n",
    "\n",
    "    stock_collection_data: pd.DataFrame = pd.read_pickle(stock_collection_data_path)\n",
    "    stock_collection_data.rename(columns={'stock_date': 'invoice_date'}, inplace=True)\n",
    "    stock_collection_data['pos_code'] = stock_collection_data['pos_code'].astype(str)\n",
    "    print('Completed loading stock collection data.')\n",
    "\n",
    "    credit_requests_data: pd.DataFrame = pd.read_pickle(credit_requests_data_path)\n",
    "    credit_requests_data = credit_requests_data[take_credit_req_data_cols]\n",
    "    credit_requests_data = __prep_credit_req_data(credit_requests_data, pred_start_date)\n",
    "    print('Completed loading credit requests data.')\n",
    "\n",
    "    pre_easter_effect_data: pd.DataFrame = pd.read_pickle(pre_easter_effect_data_path)\n",
    "    pre_easter_effect_data = pre_easter_effect_data.set_index('dates')\n",
    "    print('Completed loading pre easter effect data.')\n",
    "\n",
    "    return date_analysis_data, shipment_records, visit_plan_data, \\\n",
    "       future_visit_plan_data, pos_visit_plan_data, shipment_split_data, \\\n",
    "       sr_loading_data, sr_unloading_data, stock_collection_data, \\\n",
    "       credit_requests_data, pre_easter_effect_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed loading shipment data.\n",
      "Completed loading different date features data.\n",
      "Completed loading visit plan related data.\n",
      "Completed loading ER, non-ER shipment split data.\n",
      "Completed loading SR loading data.\n",
      "Completed loading SR unloading data.\n",
      "Completed loading stock collection data.\n",
      "Completed loading credit requests data.\n",
      "Completed loading pre easter effect data.\n"
     ]
    }
   ],
   "source": [
    "bank_holidays_df, shipment_data_df, visit_plan_df, future_visit_plan_df,\\\n",
    "pos_visit_plan_df, shipment_split_df, sr_loading_df, sr_unloading_df, stock_collection_df,\\\n",
    "credit_requests_df, pre_easter_effect_df = load_data(visit_plan_path,shipment_data_path,\n",
    "                                                     date_analysis_data_path,\n",
    "                                                     shipment_split_data_path,\n",
    "                                                     sr_loading_data_path,\n",
    "                                                     sr_unloading_data_path,\n",
    "                                                     stock_collection_data_path,\n",
    "                                                     credit_requests_data_path,\n",
    "                                                     pre_easter_effect_data_path,\n",
    "                                                     test_period_start_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "shipment_temp: pd.DataFrame = shipment_data_df.reset_index()\n",
    "shipment_temp = shipment_temp.rename(columns={'invoice_date': 'visit_date'})\n",
    "shipment_temp = shipment_temp[['visit_date', 'pos_code', 'agent_code']]\n",
    "# we need to take all poses visited by the agent on a particular date,\n",
    "# irrespective of product. So we are kind of taking the union of pos sets from all\n",
    "# products on a particular date for each agent.\n",
    "shipment_temp = shipment_temp.drop_duplicates()\n",
    "\n",
    "credit_requests_df.agent_code = credit_requests_df.agent_code.astype(str)\n",
    "credit_requests_pos_map: pd.DataFrame = pd.merge(credit_requests_df,\n",
    "                                                 shipment_temp,\n",
    "                                                 how='left',\n",
    "                                                 on=['visit_date', 'agent_code'])\n",
    "credit_requests_pos_map = credit_requests_pos_map.dropna()\n",
    "credit_requests_pos_map = credit_requests_pos_map.sort_values(\n",
    "    ['visit_date', 'increment_coeff'], ascending=False)\n",
    "credit_requests_pos_map = credit_requests_pos_map.drop_duplicates(\n",
    "    subset=['visit_date', 'pos_code'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Lap-114:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_collection_df.invoice_date = pd.to_datetime(stock_collection_df.invoice_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "list_dataframes = [(\"shipment_data\", shipment_data_df),\n",
    "                   (\"pos_visit_plan\", pos_visit_plan_df),\n",
    "                   (\"bank_holidays\", bank_holidays_df),\n",
    "                   (\"stock_collection\", stock_collection_df),\n",
    "                   (\"credit_requests\", credit_requests_pos_map),\n",
    "                   (\"pre_easter_effect\", pre_easter_effect_df)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "broadcasted_df = sc.broadcast(list_dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "visit_dates = future_visit_plan_df.reset_index().visit_date.unique()\n",
    "agent_codes = future_visit_plan_df.reset_index().agent_code.unique()\n",
    "pos_codes = future_visit_plan_df.reset_index().pos_code.unique()\n",
    "\n",
    "product_codes = shipment_data_df.product_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_code = \"TBHBK\"\n",
    "# pos_code = '10068415'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_timeseries_tasks = [] # Extract timeseries tasks\n",
    "for pos_code in pos_codes:\n",
    "# for pro_code in product_codes[:3]:\n",
    "    extract_timeseries_tasks = extract_timeseries_tasks + [(pro_code, pos_code)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_timeseries_tasks = []\n",
    "# extract_timeseries_tasks = extract_timeseries_tasks + [(pro_code, pos_code)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('TBHBK', '10067501'),\n",
       " ('TBHBK', '10067671'),\n",
       " ('TBHBK', '10067673'),\n",
       " ('TBHBK', '10067676'),\n",
       " ('TBHBK', '10067680'),\n",
       " ('TBHBK', '10067863'),\n",
       " ('TBHBK', '10068296'),\n",
       " ('TBHBK', '10068297'),\n",
       " ('TBHBK', '10068390'),\n",
       " ('TBHBK', '10068399'),\n",
       " ('TBHBK', '10068400'),\n",
       " ('TBHBK', '10068401'),\n",
       " ('TBHBK', '10068415'),\n",
       " ('TBHBK', '10068571'),\n",
       " ('TBHBK', '10068669'),\n",
       " ('TBHBK', '10068684'),\n",
       " ('TBHBK', '10068715'),\n",
       " ('TBHBK', '20761568'),\n",
       " ('TBHBK', '20919020'),\n",
       " ('TBHBK', '22347169'),\n",
       " ('TBHBK', '23199805'),\n",
       " ('TBHBK', '23555218'),\n",
       " ('TBHBK', '28008078'),\n",
       " ('TBHBK', '10030899'),\n",
       " ('TBHBK', '10067614'),\n",
       " ('TBHBK', '10067616'),\n",
       " ('TBHBK', '10067620'),\n",
       " ('TBHBK', '10067624'),\n",
       " ('TBHBK', '10067728'),\n",
       " ('TBHBK', '10068007'),\n",
       " ('TBHBK', '10068492'),\n",
       " ('TBHBK', '10068494'),\n",
       " ('TBHBK', '10068516'),\n",
       " ('TBHBK', '10068531'),\n",
       " ('TBHBK', '10068667'),\n",
       " ('TBHBK', '10068670'),\n",
       " ('TBHBK', '10068787'),\n",
       " ('TBHBK', '20069141'),\n",
       " ('TBHBK', '20347355'),\n",
       " ('TBHBK', '21942877'),\n",
       " ('TBHBK', '23441505'),\n",
       " ('TBHBK', '23502707'),\n",
       " ('TBHBK', '23506663'),\n",
       " ('TBHBK', '28000442'),\n",
       " ('TBHBK', '28016988'),\n",
       " ('TBHBK', '28020011'),\n",
       " ('TBHBK', '28021495'),\n",
       " ('TBHBK', '10018889'),\n",
       " ('TBHBK', '10029004'),\n",
       " ('TBHBK', '10067443'),\n",
       " ('TBHBK', '10067448'),\n",
       " ('TBHBK', '10067467'),\n",
       " ('TBHBK', '10067519'),\n",
       " ('TBHBK', '10067522'),\n",
       " ('TBHBK', '10067529'),\n",
       " ('TBHBK', '10067533'),\n",
       " ('TBHBK', '10067598'),\n",
       " ('TBHBK', '10067611'),\n",
       " ('TBHBK', '10067613'),\n",
       " ('TBHBK', '10067668'),\n",
       " ('TBHBK', '10067669'),\n",
       " ('TBHBK', '10068493'),\n",
       " ('TBHBK', '10068561'),\n",
       " ('TBHBK', '10068597'),\n",
       " ('TBHBK', '10068680'),\n",
       " ('TBHBK', '20069122'),\n",
       " ('TBHBK', '22367156'),\n",
       " ('TBHBK', '23244236'),\n",
       " ('TBHBK', '23347277'),\n",
       " ('TBHBK', '23501658'),\n",
       " ('TBHBK', '23581914'),\n",
       " ('TBHBK', '10017227'),\n",
       " ('TBHBK', '10018892'),\n",
       " ('TBHBK', '10067436'),\n",
       " ('TBHBK', '10067554'),\n",
       " ('TBHBK', '10067619'),\n",
       " ('TBHBK', '10067679'),\n",
       " ('TBHBK', '10067751'),\n",
       " ('TBHBK', '10068250'),\n",
       " ('TBHBK', '10068331'),\n",
       " ('TBHBK', '10068431'),\n",
       " ('TBHBK', '10068602'),\n",
       " ('TBHBK', '10068665'),\n",
       " ('TBHBK', '20592601'),\n",
       " ('TBHBK', '20795559'),\n",
       " ('TBHBK', '20918937'),\n",
       " ('TBHBK', '21815470'),\n",
       " ('TBHBK', '21868538'),\n",
       " ('TBHBK', '22071540'),\n",
       " ('TBHBK', '22346283'),\n",
       " ('TBHBK', '22735011'),\n",
       " ('TBHBK', '22736398'),\n",
       " ('TBHBK', '23085297'),\n",
       " ('TBHBK', '23378499'),\n",
       " ('TBHBK', '23378779'),\n",
       " ('TBHBK', '23379190'),\n",
       " ('TBHBK', '23379276'),\n",
       " ('TBHBK', '28000419'),\n",
       " ('TBHBK', '28016943'),\n",
       " ('TBHBK', '28018980'),\n",
       " ('TBHBK', '28019494'),\n",
       " ('TBHBK', '28022209'),\n",
       " ('TBHBK', '28022854'),\n",
       " ('TBHBK', '10018926'),\n",
       " ('TBHBK', '10067621'),\n",
       " ('TBHBK', '10067623'),\n",
       " ('TBHBK', '10067626'),\n",
       " ('TBHBK', '10067709'),\n",
       " ('TBHBK', '10067727'),\n",
       " ('TBHBK', '10067825'),\n",
       " ('TBHBK', '10068024'),\n",
       " ('TBHBK', '10068025'),\n",
       " ('TBHBK', '10068549'),\n",
       " ('TBHBK', '10068644'),\n",
       " ('TBHBK', '10112777'),\n",
       " ('TBHBK', '20779308'),\n",
       " ('TBHBK', '21943350'),\n",
       " ('TBHBK', '22073943'),\n",
       " ('TBHBK', '22365990'),\n",
       " ('TBHBK', '22366562'),\n",
       " ('TBHBK', '23378466'),\n",
       " ('TBHBK', '28012974'),\n",
       " ('TBHBK', '28017039'),\n",
       " ('TBHBK', '10067736'),\n",
       " ('TBHBK', '10068328'),\n",
       " ('TBHBK', '10021394'),\n",
       " ('TBHBK', '10067424'),\n",
       " ('TBHBK', '10068213'),\n",
       " ('TBHBK', '10068263'),\n",
       " ('TBHBK', '10068311'),\n",
       " ('TBHBK', '10068512'),\n",
       " ('TBHBK', '10068682'),\n",
       " ('TBHBK', '10068693'),\n",
       " ('TBHBK', '20069202'),\n",
       " ('TBHBK', '20069214'),\n",
       " ('TBHBK', '20069215'),\n",
       " ('TBHBK', '20592633'),\n",
       " ('TBHBK', '20779389'),\n",
       " ('TBHBK', '21942644'),\n",
       " ('TBHBK', '22367241'),\n",
       " ('TBHBK', '22367244'),\n",
       " ('TBHBK', '23150376'),\n",
       " ('TBHBK', '28008079'),\n",
       " ('TBHBK', '10067820'),\n",
       " ('TBHBK', '10017222'),\n",
       " ('TBHBK', '10067431'),\n",
       " ('TBHBK', '10067471'),\n",
       " ('TBHBK', '10067514'),\n",
       " ('TBHBK', '10067515'),\n",
       " ('TBHBK', '10067560'),\n",
       " ('TBHBK', '10068239'),\n",
       " ('TBHBK', '10068253'),\n",
       " ('TBHBK', '10068586'),\n",
       " ('TBHBK', '10068591'),\n",
       " ('TBHBK', '10068658'),\n",
       " ('TBHBK', '10068685'),\n",
       " ('TBHBK', '20592632'),\n",
       " ('TBHBK', '21079131'),\n",
       " ('TBHBK', '21797273'),\n",
       " ('TBHBK', '21816074'),\n",
       " ('TBHBK', '22347248'),\n",
       " ('TBHBK', '22667296'),\n",
       " ('TBHBK', '22757075'),\n",
       " ('TBHBK', '23469806'),\n",
       " ('TBHBK', '23555215'),\n",
       " ('TBHBK', '23602334'),\n",
       " ('TBHBK', '23627759'),\n",
       " ('TBHBK', '10011525'),\n",
       " ('TBHBK', '10017374'),\n",
       " ('TBHBK', '10021404'),\n",
       " ('TBHBK', '10067414'),\n",
       " ('TBHBK', '10067419'),\n",
       " ('TBHBK', '10067486'),\n",
       " ('TBHBK', '10067612'),\n",
       " ('TBHBK', '10067689'),\n",
       " ('TBHBK', '10067690'),\n",
       " ('TBHBK', '10067692'),\n",
       " ('TBHBK', '10067721'),\n",
       " ('TBHBK', '10068337'),\n",
       " ('TBHBK', '10068407'),\n",
       " ('TBHBK', '10068421'),\n",
       " ('TBHBK', '10068450'),\n",
       " ('TBHBK', '10068716'),\n",
       " ('TBHBK', '20069101'),\n",
       " ('TBHBK', '20069182'),\n",
       " ('TBHBK', '20330522'),\n",
       " ('TBHBK', '21905704'),\n",
       " ('TBHBK', '22075461'),\n",
       " ('TBHBK', '22551907'),\n",
       " ('TBHBK', '23442272'),\n",
       " ('TBHBK', '28019659'),\n",
       " ('TBHBK', '28020012'),\n",
       " ('TBHBK', '10067516'),\n",
       " ('TBHBK', '10067538'),\n",
       " ('TBHBK', '10067572'),\n",
       " ('TBHBK', '10067578'),\n",
       " ('TBHBK', '10067601'),\n",
       " ('TBHBK', '10067607'),\n",
       " ('TBHBK', '10067710'),\n",
       " ('TBHBK', '10067720'),\n",
       " ('TBHBK', '10068013'),\n",
       " ('TBHBK', '10068254'),\n",
       " ('TBHBK', '10068341'),\n",
       " ('TBHBK', '10068352'),\n",
       " ('TBHBK', '10068428'),\n",
       " ('TBHBK', '10068599'),\n",
       " ('TBHBK', '10068744'),\n",
       " ('TBHBK', '10068745'),\n",
       " ('TBHBK', '20346414'),\n",
       " ('TBHBK', '21078133'),\n",
       " ('TBHBK', '21905666'),\n",
       " ('TBHBK', '22346606'),\n",
       " ('TBHBK', '22757385'),\n",
       " ('TBHBK', '23442396'),\n",
       " ('TBHBK', '10029097'),\n",
       " ('TBHBK', '10067401'),\n",
       " ('TBHBK', '10067407'),\n",
       " ('TBHBK', '10067410'),\n",
       " ('TBHBK', '10067420'),\n",
       " ('TBHBK', '10067551'),\n",
       " ('TBHBK', '10067602'),\n",
       " ('TBHBK', '10067608'),\n",
       " ('TBHBK', '10067788'),\n",
       " ('TBHBK', '10067979'),\n",
       " ('TBHBK', '10068016'),\n",
       " ('TBHBK', '10068194'),\n",
       " ('TBHBK', '10068203'),\n",
       " ('TBHBK', '10068249'),\n",
       " ('TBHBK', '10068286'),\n",
       " ('TBHBK', '10068384'),\n",
       " ('TBHBK', '10068587'),\n",
       " ('TBHBK', '10068588'),\n",
       " ('TBHBK', '10068746'),\n",
       " ('TBHBK', '10068783'),\n",
       " ('TBHBK', '10068784'),\n",
       " ('TBHBK', '20919023'),\n",
       " ('TBHBK', '21078344'),\n",
       " ('TBHBK', '22712029'),\n",
       " ('TBHBK', '22736397'),\n",
       " ('TBHBK', '22757527'),\n",
       " ('TBHBK', '23501199'),\n",
       " ('TBHBK', '10017410'),\n",
       " ('TBHBK', '10067494'),\n",
       " ('TBHBK', '10067495'),\n",
       " ('TBHBK', '10067505'),\n",
       " ('TBHBK', '10067546'),\n",
       " ('TBHBK', '10067562'),\n",
       " ('TBHBK', '10067568'),\n",
       " ('TBHBK', '10067580'),\n",
       " ('TBHBK', '10068280'),\n",
       " ('TBHBK', '10068434'),\n",
       " ('TBHBK', '10068690'),\n",
       " ('TBHBK', '10068727'),\n",
       " ('TBHBK', '21077959'),\n",
       " ('TBHBK', '22023841'),\n",
       " ('TBHBK', '22024330'),\n",
       " ('TBHBK', '22346659'),\n",
       " ('TBHBK', '23349344'),\n",
       " ('TBHBK', '23627192'),\n",
       " ('TBHBK', '28023231'),\n",
       " ('TBHBK', '10021458'),\n",
       " ('TBHBK', '10029005'),\n",
       " ('TBHBK', '10067594'),\n",
       " ('TBHBK', '10067687'),\n",
       " ('TBHBK', '10067705'),\n",
       " ('TBHBK', '10068126'),\n",
       " ('TBHBK', '10068206'),\n",
       " ('TBHBK', '10068329'),\n",
       " ('TBHBK', '10068536'),\n",
       " ('TBHBK', '10068556'),\n",
       " ('TBHBK', '10068633'),\n",
       " ('TBHBK', '20069210'),\n",
       " ('TBHBK', '20069211'),\n",
       " ('TBHBK', '20069212'),\n",
       " ('TBHBK', '20347393'),\n",
       " ('TBHBK', '22365798'),\n",
       " ('TBHBK', '22667281'),\n",
       " ('TBHBK', '23199801'),\n",
       " ('TBHBK', '23349231'),\n",
       " ('TBHBK', '23359889'),\n",
       " ('TBHBK', '23447925'),\n",
       " ('TBHBK', '23530468'),\n",
       " ('TBHBK', '23557218'),\n",
       " ('TBHBK', '10067465'),\n",
       " ('TBHBK', '10067528'),\n",
       " ('TBHBK', '10067545'),\n",
       " ('TBHBK', '10067583'),\n",
       " ('TBHBK', '10068207'),\n",
       " ('TBHBK', '10068243'),\n",
       " ('TBHBK', '10068495'),\n",
       " ('TBHBK', '10068496'),\n",
       " ('TBHBK', '10068627'),\n",
       " ('TBHBK', '10068656'),\n",
       " ('TBHBK', '20169334'),\n",
       " ('TBHBK', '20617794'),\n",
       " ('TBHBK', '20955632'),\n",
       " ('TBHBK', '22043139'),\n",
       " ('TBHBK', '22097459'),\n",
       " ('TBHBK', '22327630'),\n",
       " ('TBHBK', '22711334'),\n",
       " ('TBHBK', '22778234'),\n",
       " ('TBHBK', '23147233'),\n",
       " ('TBHBK', '10067485'),\n",
       " ('TBHBK', '10067565'),\n",
       " ('TBHBK', '10067579'),\n",
       " ('TBHBK', '10068281'),\n",
       " ('TBHBK', '10068342'),\n",
       " ('TBHBK', '10068562'),\n",
       " ('TBHBK', '10068709'),\n",
       " ('TBHBK', '20069143'),\n",
       " ('TBHBK', '20069205'),\n",
       " ('TBHBK', '20069206'),\n",
       " ('TBHBK', '20069207'),\n",
       " ('TBHBK', '20069208'),\n",
       " ('TBHBK', '20069209'),\n",
       " ('TBHBK', '21815815'),\n",
       " ('TBHBK', '22073850'),\n",
       " ('TBHBK', '22075248'),\n",
       " ('TBHBK', '22736342'),\n",
       " ('TBHBK', '10067405'),\n",
       " ('TBHBK', '10067406'),\n",
       " ('TBHBK', '10067511'),\n",
       " ('TBHBK', '10067512'),\n",
       " ('TBHBK', '10067606'),\n",
       " ('TBHBK', '10067636'),\n",
       " ('TBHBK', '10067637'),\n",
       " ('TBHBK', '10067639'),\n",
       " ('TBHBK', '10067640'),\n",
       " ('TBHBK', '10067642'),\n",
       " ('TBHBK', '10067645'),\n",
       " ('TBHBK', '10067646'),\n",
       " ('TBHBK', '10067647'),\n",
       " ('TBHBK', '10067648'),\n",
       " ('TBHBK', '10067649'),\n",
       " ('TBHBK', '10067650'),\n",
       " ('TBHBK', '10067652'),\n",
       " ('TBHBK', '10067653'),\n",
       " ('TBHBK', '10067658'),\n",
       " ('TBHBK', '10067659'),\n",
       " ('TBHBK', '10067661'),\n",
       " ('TBHBK', '10067665'),\n",
       " ('TBHBK', '10068373'),\n",
       " ('TBHBK', '10068374'),\n",
       " ('TBHBK', '10068466'),\n",
       " ('TBHBK', '21059233'),\n",
       " ('TBHBK', '21097438'),\n",
       " ('TBHBK', '21097439'),\n",
       " ('TBHBK', '21097440'),\n",
       " ('TBHBK', '21942871'),\n",
       " ('TBHBK', '22195085'),\n",
       " ('TBHBK', '23507090'),\n",
       " ('TBHBK', '10011526'),\n",
       " ('TBHBK', '10018950'),\n",
       " ('TBHBK', '10067534'),\n",
       " ('TBHBK', '10067688'),\n",
       " ('TBHBK', '10067704'),\n",
       " ('TBHBK', '10068256'),\n",
       " ('TBHBK', '10068260'),\n",
       " ('TBHBK', '10068402'),\n",
       " ('TBHBK', '10068427'),\n",
       " ('TBHBK', '10068529'),\n",
       " ('TBHBK', '10068564'),\n",
       " ('TBHBK', '10068688'),\n",
       " ('TBHBK', '10068691'),\n",
       " ('TBHBK', '10068699'),\n",
       " ('TBHBK', '10068703'),\n",
       " ('TBHBK', '10068723'),\n",
       " ('TBHBK', '20069201'),\n",
       " ('TBHBK', '20069203'),\n",
       " ('TBHBK', '21868121'),\n",
       " ('TBHBK', '23147244'),\n",
       " ('TBHBK', '10067456'),\n",
       " ('TBHBK', '10067632'),\n",
       " ('TBHBK', '10067633'),\n",
       " ('TBHBK', '10068350'),\n",
       " ('TBHBK', '10068411'),\n",
       " ('TBHBK', '10068532'),\n",
       " ('TBHBK', '10068566'),\n",
       " ('TBHBK', '10068663'),\n",
       " ('TBHBK', '10068673'),\n",
       " ('TBHBK', '20617503'),\n",
       " ('TBHBK', '20779449'),\n",
       " ('TBHBK', '20895252'),\n",
       " ('TBHBK', '21797245'),\n",
       " ('TBHBK', '22042603'),\n",
       " ('TBHBK', '22757171'),\n",
       " ('TBHBK', '23349487'),\n",
       " ('TBHBK', '28022742'),\n",
       " ('TBHBK', '10018984'),\n",
       " ('TBHBK', '10019020'),\n",
       " ('TBHBK', '10021418'),\n",
       " ('TBHBK', '10067440'),\n",
       " ('TBHBK', '10067488'),\n",
       " ('TBHBK', '10067489'),\n",
       " ('TBHBK', '10067490'),\n",
       " ('TBHBK', '10067694'),\n",
       " ('TBHBK', '10067718'),\n",
       " ('TBHBK', '10068201'),\n",
       " ('TBHBK', '10068225'),\n",
       " ('TBHBK', '10068228'),\n",
       " ('TBHBK', '10068231'),\n",
       " ('TBHBK', '10068288'),\n",
       " ('TBHBK', '10068347'),\n",
       " ('TBHBK', '10068383'),\n",
       " ('TBHBK', '10068579'),\n",
       " ('TBHBK', '10068692'),\n",
       " ('TBHBK', '10068740'),\n",
       " ('TBHBK', '20761464'),\n",
       " ('TBHBK', '21942878'),\n",
       " ('TBHBK', '22711294'),\n",
       " ('TBHBK', '22778192'),\n",
       " ('TBHBK', '23221174'),\n",
       " ('TBHBK', '23349411'),\n",
       " ('TBHBK', '23441407'),\n",
       " ('TBHBK', '28000418'),\n",
       " ('TBHBK', '28008076'),\n",
       " ('TBHBK', '28021511'),\n",
       " ('TBHBK', '10067396'),\n",
       " ('TBHBK', '10067526'),\n",
       " ('TBHBK', '10067588'),\n",
       " ('TBHBK', '10068412'),\n",
       " ('TBHBK', '10068413'),\n",
       " ('TBHBK', '10068429'),\n",
       " ('TBHBK', '10068448'),\n",
       " ('TBHBK', '20592454'),\n",
       " ('TBHBK', '20592885'),\n",
       " ('TBHBK', '21868719'),\n",
       " ('TBHBK', '21868948'),\n",
       " ('TBHBK', '21941700'),\n",
       " ('TBHBK', '10067644'),\n",
       " ('TBHBK', '10067651'),\n",
       " ('TBHBK', '10067657'),\n",
       " ('TBHBK', '10067660'),\n",
       " ('TBHBK', '10067662'),\n",
       " ('TBHBK', '10068295'),\n",
       " ('TBHBK', '10068375'),\n",
       " ('TBHBK', '10068410'),\n",
       " ('TBHBK', '10068558'),\n",
       " ('TBHBK', '21694620'),\n",
       " ('TBHBK', '21905955'),\n",
       " ('TBHBK', '23532128'),\n",
       " ('TBHBK', '10018886'),\n",
       " ('TBHBK', '10029006'),\n",
       " ('TBHBK', '10067493'),\n",
       " ('TBHBK', '10067571'),\n",
       " ('TBHBK', '10067682'),\n",
       " ('TBHBK', '10067683'),\n",
       " ('TBHBK', '10068192'),\n",
       " ('TBHBK', '10068393'),\n",
       " ('TBHBK', '10068712'),\n",
       " ('TBHBK', '10068735'),\n",
       " ('TBHBK', '20843654'),\n",
       " ('TBHBK', '20843657'),\n",
       " ('TBHBK', '21059478'),\n",
       " ('TBHBK', '21815806'),\n",
       " ('TBHBK', '10018851'),\n",
       " ('TBHBK', '10067399'),\n",
       " ('TBHBK', '10067442'),\n",
       " ('TBHBK', '10067446'),\n",
       " ('TBHBK', '10067457'),\n",
       " ('TBHBK', '10067459'),\n",
       " ('TBHBK', '10067504'),\n",
       " ('TBHBK', '10067530'),\n",
       " ('TBHBK', '10067730'),\n",
       " ('TBHBK', '10068198'),\n",
       " ('TBHBK', '10068226'),\n",
       " ('TBHBK', '10068465'),\n",
       " ('TBHBK', '10068593'),\n",
       " ('TBHBK', '10068781'),\n",
       " ('TBHBK', '20069213'),\n",
       " ('TBHBK', '20330416'),\n",
       " ('TBHBK', '21815253'),\n",
       " ('TBHBK', '21868016'),\n",
       " ('TBHBK', '22024015'),\n",
       " ('TBHBK', '23379189'),\n",
       " ('TBHBK', '10018942'),\n",
       " ('TBHBK', '10029021'),\n",
       " ('TBHBK', '10067412'),\n",
       " ('TBHBK', '10067429'),\n",
       " ('TBHBK', '10067434'),\n",
       " ('TBHBK', '10068294'),\n",
       " ('TBHBK', '10068526'),\n",
       " ('TBHBK', '21076922'),\n",
       " ('TBHBK', '22075307'),\n",
       " ('TBHBK', '22563586'),\n",
       " ('TBHBK', '23448055'),\n",
       " ('TBHBK', '23530473'),\n",
       " ('TBHBK', '28008077'),\n",
       " ('TBHBK', '28019509'),\n",
       " ('TBHBK', '28023228'),\n",
       " ('TBHBK', '10030903'),\n",
       " ('TBHBK', '10067428'),\n",
       " ('TBHBK', '10067510'),\n",
       " ('TBHBK', '10067539'),\n",
       " ('TBHBK', '10067599'),\n",
       " ('TBHBK', '10068600'),\n",
       " ('TBHBK', '20285830'),\n",
       " ('TBHBK', '20590769'),\n",
       " ('TBHBK', '22551980'),\n",
       " ('TBHBK', '23349138'),\n",
       " ('TBHBK', '23652492'),\n",
       " ('TBHBK', '10068366'),\n",
       " ('TBHBK', '10068376'),\n",
       " ('TBHBK', '10068377'),\n",
       " ('TBHBK', '10068572'),\n",
       " ('TBHBK', '10068573'),\n",
       " ('TBHBK', '23085336'),\n",
       " ('TBHBK', '28023770'),\n",
       " ('TBHBK', '23653731'),\n",
       " ('TBHBK', '10068662'),\n",
       " ('TBHBK', '23786349'),\n",
       " ('TBHBK', '28016515'),\n",
       " ('TBHBK', '24414854'),\n",
       " ('TBHBK', '10044824'),\n",
       " ('TBHBK', '10067819'),\n",
       " ('TBHBK', '28016979'),\n",
       " ('TBHBK', '28024078'),\n",
       " ('TBHBK', '23150387'),\n",
       " ('TBHBK', '28004670'),\n",
       " ('TBHBK', '28024355'),\n",
       " ('TBHBK', '10068717'),\n",
       " ('TBHBK', '24445443'),\n",
       " ('TBHBK', '10060151'),\n",
       " ('TBHBK', '10060163'),\n",
       " ('TBHBK', '10111935'),\n",
       " ('TBHBK', '10111936'),\n",
       " ('TBHBK', '10111937'),\n",
       " ('TBHBK', '10111938'),\n",
       " ('TBHBK', '10111939'),\n",
       " ('TBHBK', '10112754'),\n",
       " ('TBHBK', '10112756'),\n",
       " ('TBHBK', '10112757'),\n",
       " ('TBHBK', '10112758'),\n",
       " ('TBHBK', '10112759'),\n",
       " ('TBHBK', '10112760'),\n",
       " ('TBHBK', '10112761'),\n",
       " ('TBHBK', '10112762'),\n",
       " ('TBHBK', '10112763'),\n",
       " ('TBHBK', '10112764'),\n",
       " ('TBHBK', '10112765'),\n",
       " ('TBHBK', '10112768'),\n",
       " ('TBHBK', '10112773'),\n",
       " ('TBHBK', '10112774'),\n",
       " ('TBHBK', '10112776'),\n",
       " ('TBHBK', '10112778'),\n",
       " ('TBHBK', '10112780'),\n",
       " ('TBHBK', '21816519'),\n",
       " ('TBHBK', '28014785'),\n",
       " ('TBHBK', '24477115'),\n",
       " ('TBHBK', '22077521'),\n",
       " ('TBHBK', '24534785'),\n",
       " ('TBHBK', '24534195'),\n",
       " ('TBHBK', '24535574'),\n",
       " ('TBHBK', '24535441'),\n",
       " ('TBHBK', '24535592'),\n",
       " ('TBHBK', '24536733'),\n",
       " ('TBHBK', '24536734'),\n",
       " ('TBHBK', '28020005'),\n",
       " ('TBHBK', '24536724'),\n",
       " ('TBHBK', '24536959'),\n",
       " ('TBHBK', '24537715'),\n",
       " ('TBHBK', '24537995'),\n",
       " ('TBHBK', '24538008'),\n",
       " ('TBHBK', '24538011'),\n",
       " ('TBHBK', '24538012'),\n",
       " ('TBHBK', '24566365'),\n",
       " ('TBHBK', '24537143'),\n",
       " ('TBHBK', '24538069'),\n",
       " ('TBHBK', '28024969'),\n",
       " ('TBHBK', '24566532'),\n",
       " ('TBHBK', '24566801'),\n",
       " ('TBHBK', '24566889'),\n",
       " ('TBHBK', '24569439'),\n",
       " ('TBHBK', '24569671'),\n",
       " ('TBHBK', '28025011'),\n",
       " ('TBHBK', '10067666'),\n",
       " ('TBHBK', '24598700'),\n",
       " ('TBHBK', '28025071'),\n",
       " ('TBHBK', '24599178'),\n",
       " ('TBHBK', '28025117'),\n",
       " ('TBHBK', '24600004'),\n",
       " ('TBHBK', '24627931'),\n",
       " ('TBHBK', '24628223'),\n",
       " ('TBHBK', '24628267'),\n",
       " ('TBHBK', '24628268'),\n",
       " ('TBHBK', '24628890'),\n",
       " ('TBHBK', '28025215'),\n",
       " ('TBHBK', '24657517'),\n",
       " ('TBHBK', '24657646'),\n",
       " ('TBHBK', '24657896'),\n",
       " ('TBHBK', '24657565'),\n",
       " ('TBHBK', '24657898'),\n",
       " ('TBHBK', '24658137')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_timeseries_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def extract_ts(pro_code, pos_code):\n",
    "    shipment_data_local = broadcasted_df.value[0][1].copy()\n",
    "    pos_visit_plan_data = broadcasted_df.value[1][1].copy()\n",
    "    inflated_demand_marker_df = broadcasted_df.value[2][1].copy()\n",
    "    stock_collection_data = broadcasted_df.value[3][1].copy()\n",
    "    credit_requests_data = broadcasted_df.value[4][1].copy()\n",
    "    pre_easter_effect_data = broadcasted_df.value[5][1].copy()\n",
    "\n",
    "    inflated_demand_marker_df.index = pd.to_datetime(inflated_demand_marker_df.index)\n",
    "\n",
    "    # pos_visit_plan_data.index = pos_visit_plan_data.index.astype(int)\n",
    "    pos_visit_plan_data.visit_date = pd.to_datetime(pos_visit_plan_data.visit_date)\n",
    "\n",
    "    # stock_collection_data.invoice_date = pd.to_datetime(stock_collection_data.invoice_date)\n",
    "    # pre_easter_effect_data.index = pd.to_datetime(pre_easter_effect_data.index)\n",
    "\n",
    "    last_visit_date = pos_visit_plan_data[pos_visit_plan_data.index == pos_code].visit_date.max()\n",
    "\n",
    "    pos_pro_df = shipment_data_local[(shipment_data_local.index <= pd.to_datetime(last_visit_date)) & \n",
    "                                         (shipment_data_local.pos_code == pos_code) & \n",
    "                                         (shipment_data_local.product_code == pro_code)]\n",
    "\n",
    "    if len(pos_pro_df):\n",
    "        pos_pro_df.pos_code = pos_pro_df.pos_code.astype(int)\n",
    "        credit_requests_data.pos_code = credit_requests_data.pos_code.astype(int)\n",
    "\n",
    "        current_pos_code = pos_code\n",
    "        visit_plan_dates = pos_visit_plan_data[pos_visit_plan_data.index == current_pos_code]\n",
    "        visit_plan_dates = visit_plan_dates.reset_index().set_index('visit_date')\n",
    "        visit_plan_dates = visit_plan_dates[~visit_plan_dates.index.duplicated(keep='last')]\n",
    "        visit_plan_dates['plan_date'] = True\n",
    "\n",
    "        current_product_code = pro_code\n",
    "        stock_df = stock_collection_data.loc[\n",
    "            (stock_collection_data['pos_code'] == current_pos_code) &\n",
    "            (stock_collection_data['product_code'] == current_product_code)]\n",
    "        stock_df = stock_df.set_index('invoice_date')\n",
    "\n",
    "        credit_requests_df = credit_requests_data.loc[\n",
    "            credit_requests_data['pos_code'] == current_pos_code]\n",
    "        credit_requests_df = credit_requests_df.set_index('visit_date')\n",
    "\n",
    "        pos_pro_df.sort_index(inplace=True)\n",
    "\n",
    "        pos_pro_df.rename(columns={'quantity': 'shipments'}, inplace=True)\n",
    "\n",
    "        first_nonzero_sale_date = pos_pro_df.index[0]\n",
    "        if len(pos_pro_df) < 2:\n",
    "            second_nonzero_sale_date = None\n",
    "        else:\n",
    "            second_nonzero_sale_date = pos_pro_df.index[1]\n",
    "        if len(pos_pro_df) < 3:\n",
    "            third_nonzero_sale_date = None\n",
    "        else:\n",
    "            third_nonzero_sale_date = pos_pro_df.index[2]\n",
    "        if len(pos_pro_df) < 4:\n",
    "            fourth_nonzero_sale_date = None\n",
    "        else:\n",
    "            fourth_nonzero_sale_date = pos_pro_df.index[3]\n",
    "        if len(pos_pro_df) < 5:\n",
    "            fifth_nonzero_sale_date = None\n",
    "        else:\n",
    "            fifth_nonzero_sale_date = pos_pro_df.index[4]\n",
    "        if len(pos_pro_df) < 6:\n",
    "            sixth_nonzero_sale_date = None\n",
    "        else:\n",
    "            sixth_nonzero_sale_date = pos_pro_df.index[5]\n",
    "        nonzero_shipment_dates: pd.Series = pd.Series(pos_pro_df.index)\n",
    "        pos_pro_df.at[pos_pro_df.index[0], 'is_first_nonzero_sale_date'] = True\n",
    "        pos_pro_df.at[pos_pro_df.index[-1], 'is_last_sale_date'] = True\n",
    "        pos_pro_df['nonzero_Shipments_1'] = pos_pro_df['shipments'].shift(1)\n",
    "        overflow_shipment_1 = pos_pro_df['shipments'].iloc[-1]\n",
    "        pos_pro_df['nonzero_Shipments_2'] = pos_pro_df['nonzero_Shipments_1'].shift(1)\n",
    "        overflow_shipment_2 = pos_pro_df['nonzero_Shipments_1'].iloc[-1]\n",
    "        pos_pro_df['nonzero_Shipments_3'] = pos_pro_df['nonzero_Shipments_2'].shift(1)\n",
    "        overflow_shipment_3 = pos_pro_df['nonzero_Shipments_2'].iloc[-1]\n",
    "        pos_pro_df['nonzero_Shipments_4'] = pos_pro_df['nonzero_Shipments_3'].shift(1)\n",
    "        overflow_shipment_4 = pos_pro_df['nonzero_Shipments_3'].iloc[-1]\n",
    "        pos_pro_df['nonzero_Shipments_5'] = pos_pro_df['nonzero_Shipments_4'].shift(1)\n",
    "        overflow_shipment_5 = pos_pro_df['nonzero_Shipments_4'].iloc[-1]\n",
    "        pos_pro_df['nonzero_Shipments_6'] = pos_pro_df['nonzero_Shipments_5'].shift(1)\n",
    "        overflow_shipment_6 = pos_pro_df['nonzero_Shipments_5'].iloc[-1]\n",
    "        pos_pro_df['days_since_last_nonzero_sale'] = nonzero_shipment_dates.diff().dt.days.values\n",
    "        pos_pro_df['days_since_last_nonzero_sale_1'] = (nonzero_shipment_dates.shift(1)\n",
    "                                                        - nonzero_shipment_dates.shift(2)).dt.days.values\n",
    "        overflow_days_since_last_nonzero_sale_1 = pos_pro_df['days_since_last_nonzero_sale'].iloc[-1]\n",
    "        pos_pro_df['days_since_last_nonzero_sale_2'] = (nonzero_shipment_dates.shift(2)\n",
    "                                                        - nonzero_shipment_dates.shift(3)).dt.days.values\n",
    "        overflow_days_since_last_nonzero_sale_2 = pos_pro_df[\n",
    "            'days_since_last_nonzero_sale'].shift(1).iloc[-1]\n",
    "        pos_pro_df['days_since_last_nonzero_sale_3'] = (nonzero_shipment_dates.shift(3)\n",
    "                                                        - nonzero_shipment_dates.shift(4)).dt.days.values\n",
    "        overflow_days_since_last_nonzero_sale_3 = pos_pro_df[\n",
    "            'days_since_last_nonzero_sale'].shift(2).iloc[-1]\n",
    "        # print(pos_pro_df.head(10))\n",
    "\n",
    "        new_index = visit_plan_dates.index.union(pos_pro_df.index)\n",
    "        new_index = new_index.union([last_visit_date])\n",
    "        pos_pro_df = pos_pro_df.reindex(new_index)\n",
    "\n",
    "        global preprocess_constant_cols\n",
    "        pos_pro_df.loc[:, preprocess_constant_cols] = \\\n",
    "            pos_pro_df.loc[:, preprocess_constant_cols].fillna(method='ffill')\n",
    "        pos_pro_df.loc[:, preprocess_constant_cols] = \\\n",
    "            pos_pro_df.loc[:, preprocess_constant_cols].fillna(method='bfill')\n",
    "        pos_pro_df.index.names = ['invoice_date']\n",
    "\n",
    "        pos_pro_df['is_first_nonzero_sale_date'] = pos_pro_df['is_first_nonzero_sale_date'].fillna(False)\n",
    "        pos_pro_df['is_last_sale_date'] = pos_pro_df['is_last_sale_date'].fillna(False)\n",
    "        pos_pro_df['nonzero_Shipments_1'] = pos_pro_df['nonzero_Shipments_1'].fillna(method='bfill')\n",
    "        pos_pro_df.loc[pos_pro_df.index <= first_nonzero_sale_date,\n",
    "                       'nonzero_Shipments_1'] = 0\n",
    "        pos_pro_df['nonzero_Shipments_1'] = pos_pro_df['nonzero_Shipments_1'].fillna(overflow_shipment_1)\n",
    "        pos_pro_df.loc[pos_pro_df.index <= first_nonzero_sale_date,\n",
    "                       'days_since_last_nonzero_sale'] = -1\n",
    "        if second_nonzero_sale_date:\n",
    "            pos_pro_df['nonzero_Shipments_2'] = pos_pro_df['nonzero_Shipments_2'].fillna(method='bfill')\n",
    "            pos_pro_df.loc[pos_pro_df.index <= second_nonzero_sale_date,\n",
    "                           'nonzero_Shipments_2'] = 0\n",
    "            pos_pro_df['nonzero_Shipments_2'] = pos_pro_df['nonzero_Shipments_2'].fillna(overflow_shipment_2)\n",
    "            pos_pro_df['days_since_last_nonzero_sale_1'] = \\\n",
    "                pos_pro_df['days_since_last_nonzero_sale_1'].fillna(method='bfill')\n",
    "            pos_pro_df.loc[pos_pro_df.index <= second_nonzero_sale_date,\n",
    "                           'days_since_last_nonzero_sale_1'] = -1\n",
    "            pos_pro_df['days_since_last_nonzero_sale_1'] = \\\n",
    "                pos_pro_df['days_since_last_nonzero_sale_1'].fillna(overflow_days_since_last_nonzero_sale_1)\n",
    "        else:\n",
    "            pos_pro_df['nonzero_Shipments_2'] = 0\n",
    "            pos_pro_df['days_since_last_nonzero_sale_1'] = -1\n",
    "        if third_nonzero_sale_date:\n",
    "            pos_pro_df['nonzero_Shipments_3'] = pos_pro_df['nonzero_Shipments_3'].fillna(method='bfill')\n",
    "            pos_pro_df.loc[pos_pro_df.index <= third_nonzero_sale_date,\n",
    "                           'nonzero_Shipments_3'] = 0\n",
    "            pos_pro_df['nonzero_Shipments_3'] = pos_pro_df['nonzero_Shipments_3'].fillna(overflow_shipment_3)\n",
    "            pos_pro_df['days_since_last_nonzero_sale_2'] = \\\n",
    "                pos_pro_df['days_since_last_nonzero_sale_2'].fillna(method='bfill')\n",
    "            pos_pro_df.loc[pos_pro_df.index <= third_nonzero_sale_date,\n",
    "                           'days_since_last_nonzero_sale_2'] = -1\n",
    "            pos_pro_df['days_since_last_nonzero_sale_2'] = \\\n",
    "                pos_pro_df['days_since_last_nonzero_sale_2'].fillna(overflow_days_since_last_nonzero_sale_2)\n",
    "        else:\n",
    "            pos_pro_df['nonzero_Shipments_3'] = 0\n",
    "            pos_pro_df['days_since_last_nonzero_sale_2'] = -1\n",
    "        if fourth_nonzero_sale_date:\n",
    "            pos_pro_df['nonzero_Shipments_4'] = pos_pro_df['nonzero_Shipments_4'].fillna(method='bfill')\n",
    "            pos_pro_df.loc[pos_pro_df.index <= fourth_nonzero_sale_date,\n",
    "                           'nonzero_Shipments_4'] = 0\n",
    "            pos_pro_df['nonzero_Shipments_4'] = pos_pro_df['nonzero_Shipments_4'].fillna(overflow_shipment_4)\n",
    "            pos_pro_df['days_since_last_nonzero_sale_3'] = \\\n",
    "                pos_pro_df['days_since_last_nonzero_sale_3'].fillna(method='bfill')\n",
    "            pos_pro_df.loc[pos_pro_df.index <= fourth_nonzero_sale_date,\n",
    "                           'days_since_last_nonzero_sale_3'] = -1\n",
    "            pos_pro_df['days_since_last_nonzero_sale_3'] = \\\n",
    "                pos_pro_df['days_since_last_nonzero_sale_3'].fillna(overflow_days_since_last_nonzero_sale_3)\n",
    "        else:\n",
    "            pos_pro_df['nonzero_Shipments_4'] = 0\n",
    "            pos_pro_df['days_since_last_nonzero_sale_3'] = -1\n",
    "        if fifth_nonzero_sale_date:\n",
    "            pos_pro_df['nonzero_Shipments_5'] = pos_pro_df['nonzero_Shipments_5'].fillna(method='bfill')\n",
    "            pos_pro_df.loc[pos_pro_df.index <= fifth_nonzero_sale_date,\n",
    "                           'nonzero_Shipments_5'] = 0\n",
    "            pos_pro_df['nonzero_Shipments_5'] = pos_pro_df['nonzero_Shipments_5'].fillna(overflow_shipment_5)\n",
    "        else:\n",
    "            pos_pro_df['nonzero_Shipments_5'] = 0\n",
    "        if sixth_nonzero_sale_date:\n",
    "            pos_pro_df['nonzero_Shipments_6'] = pos_pro_df['nonzero_Shipments_6'].fillna(method='bfill')\n",
    "            pos_pro_df['nonzero_Shipments_6'] = pos_pro_df['nonzero_Shipments_6'].fillna(overflow_shipment_6)\n",
    "            pos_pro_df.loc[pos_pro_df.index <= sixth_nonzero_sale_date,\n",
    "                           'nonzero_Shipments_6'] = 0\n",
    "        else:\n",
    "            pos_pro_df['nonzero_Shipments_6'] = 0\n",
    "        pos_pro_df['return_1'] = pos_pro_df['return'].shift()\n",
    "        pos_pro_df['return_1'] = pos_pro_df['return_1'].fillna(False)\n",
    "        pos_pro_df['shipments'] = pos_pro_df['shipments'].fillna(0)\n",
    "        pos_pro_df['promo_id'] = pos_pro_df['promo_id'].fillna(0).astype('int')\n",
    "\n",
    "        pos_pro_df.at[pos_pro_df.index[0], 'is_first_plan_date'] = True\n",
    "        pos_pro_df['is_first_plan_date'] = pos_pro_df['is_first_plan_date'].fillna(False)\n",
    "        pos_pro_df['days_since_first_sale'] = (pos_pro_df.index\n",
    "                                               - pos_pro_df.index[0]).days\n",
    "        pos_pro_df['days_since_first_nonzero_sale'] = (pos_pro_df.index\n",
    "                                                       - first_nonzero_sale_date).days\n",
    "        pos_pro_df.loc[pos_pro_df.index < first_nonzero_sale_date,\n",
    "                       'days_since_first_nonzero_sale'] = -1\n",
    "        pos_pro_df['days_since_last_sale'] = pos_pro_df['days_since_first_sale'].diff().fillna(0)\n",
    "        pos_pro_df['stock'] = stock_df['stock']\n",
    "        pos_pro_df['stock'] = pos_pro_df['stock'].fillna(-999)\n",
    "        pos_pro_df['week'] = pos_pro_df.index.week\n",
    "        pos_pro_df['month'] = pos_pro_df.index.month\n",
    "        pos_pro_df['is_odd_month'] = pos_pro_df['month'].apply(lambda x: x & 1)\n",
    "        pos_pro_df['quarter'] = pos_pro_df.index.quarter\n",
    "        pos_pro_df['year'] = pos_pro_df.index.year\n",
    "        # This is the i-th year this pos is buying this product\n",
    "        pos_pro_df['year_of_engagement'] = (pos_pro_df['year']\n",
    "                                            - pos_pro_df['year'].min() + 1)\n",
    "        pos_pro_df['VisitPlanWeek'] = inflated_demand_marker_df['VisitPlanWeek']\n",
    "        pos_pro_df['DayNumberOfWeek'] = inflated_demand_marker_df['DayNumberOfWeek']\n",
    "        pos_pro_df['IsWorkingDate'] = inflated_demand_marker_df['IsWorkingDate']\n",
    "        pos_pro_df['weekday_holiday'] = inflated_demand_marker_df['WeekdayHoliday']\n",
    "        pos_pro_df['days_since_price_chg_ann'] = inflated_demand_marker_df['daysSincePriceChgAnn']\n",
    "        pos_pro_df['days_from_price_chg'] = inflated_demand_marker_df['daysFromPriceChg']\n",
    "        pos_pro_df['PricelistChg'] = inflated_demand_marker_df['PricelistChg']\n",
    "        pos_pro_df['double_sell'] = visit_plan_dates['double_sell']\n",
    "        pos_pro_df['double_sell'] = pos_pro_df['double_sell'].fillna(False)\n",
    "        pos_pro_df['triple_sell'] = visit_plan_dates['triple_sell']\n",
    "        pos_pro_df['triple_sell'] = pos_pro_df['triple_sell'].fillna(False)\n",
    "        pos_pro_df['isVisitPlan'] = visit_plan_dates['plan_date']\n",
    "        pos_pro_df['isVisitPlan'] = pos_pro_df['isVisitPlan'].fillna(False)\n",
    "        pos_pro_df['pre_nonreplacement_holiday'] = \\\n",
    "            visit_plan_dates['pre_nonreplacement_holiday']\n",
    "        pos_pro_df['pre_nonreplacement_holiday'] = pos_pro_df['pre_nonreplacement_holiday'].fillna(False)\n",
    "        pos_pro_df.loc[\n",
    "            (pos_pro_df['double_sell'] | pos_pro_df['triple_sell']),\n",
    "            'pre_nonreplacement_holiday'] = False\n",
    "        pos_pro_df['credit_request_coeff'] = credit_requests_df['increment_coeff']\n",
    "        pos_pro_df['credit_request_coeff'] = pos_pro_df['credit_request_coeff'].fillna(0)\n",
    "        pos_pro_df['credit_request_type'] = credit_requests_df['request_type']\n",
    "        pos_pro_df['credit_request_type'] = pos_pro_df['credit_request_type'].fillna('nill')\n",
    "        pos_pro_df['days_from_easter'] = pre_easter_effect_data['days_from_easter']\n",
    "        pos_pro_df['days_from_easter'] = pos_pro_df['days_from_easter'].fillna(-1)\n",
    "        pos_pro_df['isMissedPlan'] = ((pos_pro_df['stock'] == -999)\n",
    "                                      & (pos_pro_df['shipments'] == 0)\n",
    "                                      & pos_pro_df['isVisitPlan'])\n",
    "        pos_pro_df['is_zero_sale'] = (pos_pro_df['shipments'] == 0)\n",
    "        # Convert to sparse then query index to find block locations\n",
    "        is_nonstart_zero: pd.Series = pos_pro_df.loc[\n",
    "            pos_pro_df['days_since_first_nonzero_sale'] != -1,\n",
    "            'is_zero_sale']\n",
    "        temp_ts: pd.SparseSeries = is_nonstart_zero.to_sparse(fill_value=False)\n",
    "        block_locs = zip(temp_ts.sp_index.blocs, temp_ts.sp_index.blengths)\n",
    "        # Map the sparse blocks back to the dense timeseries\n",
    "        blocks = [is_nonstart_zero.iloc[start-1:(start + length)] for (start, length) in block_locs]\n",
    "        blocks = [pd.Series((block.index - block.index[0]),\n",
    "                            index=block.index).dt.days.iloc[1:]\n",
    "                  for block in blocks if not block.empty]\n",
    "        # Map the days since last nonzero sale blocks back to original df\n",
    "        for block in blocks:\n",
    "            pos_pro_df.loc[block.index, 'days_since_last_nonzero_sale'] = block\n",
    "\n",
    "        pos_pro_df['shipments_1'] = pos_pro_df['shipments'].shift(1).fillna(0)\n",
    "        pos_pro_df['days_since_last_sale_1'] = pos_pro_df['days_since_last_sale'].shift(1).fillna(-1)\n",
    "        pos_pro_df['double_sell_1'] = pos_pro_df['double_sell'].shift(1).fillna(False)\n",
    "        pos_pro_df.loc[\n",
    "            (pos_pro_df['double_sell'] | pos_pro_df['triple_sell']), 'double_sell_1'] = False\n",
    "        pos_pro_df['triple_sell_1'] = pos_pro_df['triple_sell'].shift(1).fillna(False)\n",
    "        pos_pro_df.loc[\n",
    "            (pos_pro_df['double_sell'] | pos_pro_df['triple_sell']), 'triple_sell_1'] = False\n",
    "        pos_pro_df['isVisitPlan_1'] = pos_pro_df['isVisitPlan'].shift(1).fillna(False)  # After ER\n",
    "        pos_pro_df['PricelistChg_1'] = pos_pro_df['PricelistChg'].shift(1).fillna(False)\n",
    "        pos_pro_df['return_1'] = pos_pro_df['return'].shift(1).fillna(False)  # After return\n",
    "        pos_pro_df['isMissedPlan_1'] = pos_pro_df['isMissedPlan'].shift(1).fillna(False)\n",
    "        pos_pro_df['stock_1'] = pos_pro_df['stock'].shift(1).fillna(-999)\n",
    "        pos_pro_df['is_zero_sale_1'] = pos_pro_df['is_zero_sale'].shift(1).fillna(False)\n",
    "        is_zero_1: pd.Series = pos_pro_df['is_zero_sale_1']\n",
    "        # Convert to sparse then query index to find block locations\n",
    "        temp_ts: pd.SparseSeries = is_zero_1.to_sparse(fill_value=False)\n",
    "        block_locs = zip(temp_ts.sp_index.blocs, temp_ts.sp_index.blengths)\n",
    "        # Map the sparse blocks back to the dense timeseries\n",
    "        blocks = [is_zero_1.iloc[start:(start + length)] for (start, length) in block_locs]\n",
    "        blocks = [block.astype(int).cumsum() for block in blocks if not block.empty]\n",
    "        # Map the cumsum blocks back to original df\n",
    "        pos_pro_df['num_consecutive_zero_sales'] = 0\n",
    "        for block in blocks:\n",
    "            pos_pro_df.loc[block.index, 'num_consecutive_zero_sales'] = block\n",
    "        is_nonzero_1: pd.Series = (pos_pro_df['shipments'] != 0).shift(1).fillna(False)\n",
    "        # Convert to sparse then query index to find block locations\n",
    "        temp_ts: pd.SparseSeries = is_nonzero_1.to_sparse(fill_value=False)\n",
    "        block_locs = zip(temp_ts.sp_index.blocs, temp_ts.sp_index.blengths)\n",
    "        # Map the sparse blocks back to the dense timeseries\n",
    "        blocks = [is_nonzero_1.iloc[start:(start + length)] for (start, length) in block_locs]\n",
    "        blocks = [block.astype(int).cumsum() for block in blocks if not block.empty]\n",
    "        # Map the cumsum blocks back to original df\n",
    "        pos_pro_df['num_consecutive_nonzero_sales'] = 0\n",
    "        for block in blocks:\n",
    "            pos_pro_df.loc[block.index, 'num_consecutive_nonzero_sales'] = block\n",
    "\n",
    "        pos_pro_df['double_sell_lead_1'] = pos_pro_df[\n",
    "            'double_sell'].shift(-1).fillna(False)  # Before double sell\n",
    "        pos_pro_df.loc[\n",
    "            (pos_pro_df['double_sell'] | pos_pro_df['triple_sell']\n",
    "             | pos_pro_df['double_sell_1'] | pos_pro_df['triple_sell_1']),\n",
    "            'double_sell_lead_1'] = False\n",
    "        pos_pro_df['triple_sell_lead_1'] = pos_pro_df[\n",
    "            'triple_sell'].shift(-1).fillna(False)  # Before triple sell\n",
    "        pos_pro_df.loc[\n",
    "            (pos_pro_df['double_sell'] | pos_pro_df['triple_sell']\n",
    "             | pos_pro_df['double_sell_1'] | pos_pro_df['triple_sell_1']),\n",
    "            'triple_sell_lead_1'] = False\n",
    "\n",
    "        pos_pro_df['shipments_2'] = pos_pro_df['shipments_1'].shift(1).fillna(0)\n",
    "        pos_pro_df['days_since_last_sale_2'] = pos_pro_df['days_since_last_sale_1'].shift(1).fillna(-1)\n",
    "        pos_pro_df['double_sell_2'] = pos_pro_df['double_sell_1'].shift(1).fillna(False)\n",
    "        pos_pro_df.loc[\n",
    "            (pos_pro_df['double_sell'] | pos_pro_df['triple_sell']), 'double_sell_2'] = False\n",
    "        pos_pro_df['triple_sell_2'] = pos_pro_df['triple_sell_1'].shift(1).fillna(False)\n",
    "        pos_pro_df.loc[\n",
    "            (pos_pro_df['double_sell'] | pos_pro_df['triple_sell']), 'triple_sell_2'] = False\n",
    "        pos_pro_df['isVisitPlan_2'] = pos_pro_df['isVisitPlan_1'].shift(1).fillna(False)\n",
    "        pos_pro_df['isMissedPlan_2'] = pos_pro_df['isMissedPlan_1'].shift(1).fillna(False)\n",
    "        pos_pro_df['stock_2'] = pos_pro_df['stock_1'].shift(1).fillna(-999)\n",
    "        pos_pro_df['is_zero_sale_2'] = pos_pro_df['is_zero_sale_1'].shift(1).fillna(False)\n",
    "\n",
    "        pos_pro_df['shipments_3'] = pos_pro_df['shipments_2'].shift(1).fillna(0)\n",
    "        pos_pro_df['days_since_last_sale_3'] = pos_pro_df['days_since_last_sale_2'].shift(1).fillna(-1)\n",
    "        pos_pro_df['stock_3'] = pos_pro_df['stock_2'].shift(1).fillna(-999)\n",
    "        pos_pro_df['is_zero_sale_3'] = pos_pro_df['is_zero_sale_2'].shift(1).fillna(False)\n",
    "        pos_pro_df['shipments_4'] = pos_pro_df['shipments_3'].shift(1).fillna(0)\n",
    "        pos_pro_df['stock_4'] = pos_pro_df['stock_3'].shift(1).fillna(-999)\n",
    "        pos_pro_df['shipments_5'] = pos_pro_df['shipments_4'].shift(1).fillna(0)\n",
    "        pos_pro_df['stock_5'] = pos_pro_df['stock_4'].shift(1).fillna(-999)\n",
    "        pos_pro_df['shipments_6'] = pos_pro_df['shipments_5'].shift(1).fillna(0)\n",
    "        pos_pro_df['stock_6'] = pos_pro_df['stock_5'].shift(1).fillna(-999)\n",
    "\n",
    "        past_3_shipment_labels = ['shipments_{}'.format(i) for i in range(1, 4)]\n",
    "        past_6_shipment_labels = ['shipments_{}'.format(i) for i in range(1, 7)]\n",
    "        past_3_shipments_interval_labels = ['days_since_last_sale_{}'.format(i) for i in range(1, 4)]\n",
    "        past_3_stock_labels = ['stock_{}'.format(i) for i in range(1, 4)]\n",
    "        past_6_stock_labels = ['stock_{}'.format(i) for i in range(1, 7)]\n",
    "        past_3_nonzero_shipment_labels = ['nonzero_Shipments_{}'.format(i) for i in range(1, 4)]\n",
    "        past_6_nonzero_shipment_labels = ['nonzero_Shipments_{}'.format(i) for i in range(1, 7)]\n",
    "        past_3_nonzero_shipments_interval_labels = ['days_since_last_nonzero_sale_{}'.format(i)\n",
    "                                                    for i in range(1, 4)]\n",
    "        pos_pro_df['ma_3'] = pos_pro_df[past_3_shipment_labels].mean(axis=1)\n",
    "        pos_pro_df['ma_6'] = pos_pro_df[past_6_shipment_labels].mean(axis=1)\n",
    "        pos_pro_df['mm_3'] = pos_pro_df[past_3_shipment_labels].median(axis=1)\n",
    "        pos_pro_df['mm_6'] = pos_pro_df[past_6_shipment_labels].median(axis=1)\n",
    "        pos_pro_df['ma_interval_3'] = pos_pro_df[past_3_shipments_interval_labels].mean(axis=1)\n",
    "        pos_pro_df['ma_Stock_3'] = pos_pro_df[past_3_stock_labels].mean(axis=1)\n",
    "        pos_pro_df['ma_Stock_6'] = pos_pro_df[past_6_stock_labels].mean(axis=1)\n",
    "        pos_pro_df['ma_nonzero_3'] = pos_pro_df[past_3_nonzero_shipment_labels].mean(axis=1)\n",
    "        pos_pro_df['ma_nonzero_6'] = pos_pro_df[past_6_nonzero_shipment_labels].mean(axis=1)\n",
    "        pos_pro_df['mm_nonzero_3'] = pos_pro_df[past_3_nonzero_shipment_labels].median(axis=1)\n",
    "        pos_pro_df['mm_nonzero_6'] = pos_pro_df[past_6_nonzero_shipment_labels].median(axis=1)\n",
    "        pos_pro_df['ma_nonzero_interval_3'] = \\\n",
    "            pos_pro_df[past_3_nonzero_shipments_interval_labels].mean(axis=1)\n",
    "\n",
    "        pos_pro_df['Chg_Shipments_1_from_ma_3'] = pos_pro_df['shipments_1'] - pos_pro_df['ma_3']\n",
    "        pos_pro_df['Chg_Shipments_1_from_ma_6'] = pos_pro_df['shipments_1'] - pos_pro_df['ma_6']\n",
    "        pos_pro_df['Chg_Shipments_1_from_mm_3'] = pos_pro_df['shipments_1'] - pos_pro_df['mm_3']\n",
    "        pos_pro_df['Chg_Shipments_1_from_mm_6'] = pos_pro_df['shipments_1'] - pos_pro_df['mm_6']\n",
    "        pos_pro_df['Chg_pct_Shipments_1_from_Shipments_2'] = ((pos_pro_df['shipments_1']\n",
    "                                                               - pos_pro_df['shipments_2'])\n",
    "                                                              / pos_pro_df['shipments_2']).clip(-5, 5)\n",
    "        pos_pro_df['Chg_pct_Shipments_1_from_Shipments_2'] = \\\n",
    "            pos_pro_df['Chg_pct_Shipments_1_from_Shipments_2'].fillna(-999)\n",
    "        pos_pro_df['Chg_Stock_1_from_ma_Stock_3'] = pos_pro_df['stock_1'] - pos_pro_df['ma_Stock_3']\n",
    "        pos_pro_df['Chg_Stock_1_from_ma_Stock_6'] = pos_pro_df['stock_1'] - pos_pro_df['ma_Stock_6']\n",
    "        pos_pro_df['Chg_pct_Stock_1_from_Stock_2'] = ((pos_pro_df['stock_1']\n",
    "                                                       - pos_pro_df['stock_2'])\n",
    "                                                      / pos_pro_df['stock_2']).clip(-5, 5)\n",
    "        pos_pro_df['Chg_pct_Stock_1_from_Stock_2'] = \\\n",
    "            pos_pro_df['Chg_pct_Stock_1_from_Stock_2'].fillna(-999)\n",
    "        pos_pro_df['Chg_nonzero_Shipments_1_from_ma_nonzero_3'] = \\\n",
    "            pos_pro_df['nonzero_Shipments_1'] - pos_pro_df['ma_nonzero_3']\n",
    "        pos_pro_df['Chg_nonzero_Shipments_1_from_ma_nonzero_6'] = \\\n",
    "            pos_pro_df['nonzero_Shipments_1'] - pos_pro_df['ma_nonzero_6']\n",
    "        pos_pro_df['Chg_nonzero_Shipments_1_from_mm_nonzero_3'] = \\\n",
    "            pos_pro_df['nonzero_Shipments_1'] - pos_pro_df['mm_nonzero_3']\n",
    "        pos_pro_df['Chg_nonzero_Shipments_1_from_mm_nonzero_6'] = \\\n",
    "            pos_pro_df['nonzero_Shipments_1'] - pos_pro_df['mm_nonzero_6']\n",
    "        pos_pro_df['Chg_pct_nonzero_Shipments_1_from_nonzero_Shipments_2'] = \\\n",
    "            ((pos_pro_df['nonzero_Shipments_1'] - pos_pro_df['nonzero_Shipments_2'])\n",
    "             / pos_pro_df['nonzero_Shipments_2']).clip(-5, 5)\n",
    "        pos_pro_df['Chg_pct_nonzero_Shipments_1_from_nonzero_Shipments_2'] = \\\n",
    "            pos_pro_df['Chg_pct_nonzero_Shipments_1_from_nonzero_Shipments_2'].fillna(-999)\n",
    "\n",
    "        pos_pro_df = pos_pro_df.loc[(pos_pro_df['IsWorkingDate'] == 1) &\n",
    "                                    (pos_pro_df['DayNumberOfWeek'] <= 5)]\n",
    "        pos_pro_df.drop(columns=['return', 'IsWorkingDate', 'PricelistChg'],\n",
    "                        inplace=True)\n",
    "\n",
    "        # try:\n",
    "        #     assert not pos_pro_df.isnull().any().any()\n",
    "        # except AssertionError as ae:\n",
    "        #     null_track = pos_pro_df.isnull().any()\n",
    "        #     print(null_track[null_track])\n",
    "        #     pos_pro_df.to_csv('temp.csv')\n",
    "        #     exit()\n",
    "        pos_pro_df.fillna(0, inplace=True)\n",
    "        # print(pos_pro_df.head())\n",
    "\n",
    "    return pos_pro_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "extract_ts_tasks_RDD = sc.parallelize(extract_timeseries_tasks, numSlices=len(extract_timeseries_tasks))\n",
    "extracted_ts_data = extract_ts_tasks_RDD.map(lambda v_date_pos_code: extract_ts(v_date_pos_code[0], \n",
    "                                                                                v_date_pos_code[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert RDD to Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-fab9ac9bdc99>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrdd_partition_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextracted_ts_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mcount\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1054\u001b[0m         \"\"\"\n\u001b[1;32m-> 1055\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1056\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1057\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36msum\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1044\u001b[0m         \u001b[1;36m6.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1045\u001b[0m         \"\"\"\n\u001b[1;32m-> 1046\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1047\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1048\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mfold\u001b[1;34m(self, zeroValue, op)\u001b[0m\n\u001b[0;32m    915\u001b[0m         \u001b[1;31m# zeroValue provided to each partition is unique from the one provided\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m         \u001b[1;31m# to the final reduce call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m         \u001b[0mvals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzeroValue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    814\u001b[0m         \"\"\"\n\u001b[0;32m    815\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 816\u001b[1;33m             \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    817\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1255\u001b[1;33m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 985\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    986\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command)\u001b[0m\n\u001b[0;32m   1150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1152\u001b[1;33m             \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Answer received: {0}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 589\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    590\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rdd_partition_count = extracted_ts_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_set = pd.concat(extracted_ts_data.take(rdd_partition_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_features_set.stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_set.shipments.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_set.promo_id.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_set = pd.DataFrame()\n",
    "for i in range(1,rdd_partition_count+1):\n",
    "    print(\"partition no\", i)\n",
    "    all_features_set = all_features_set.append(extracted_ts_data.take(rdd_partition_count)[i-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_features_set.to_csv(root_path + \"test_TWIBLH.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_ts_data = all_features_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_product_data = prepared_ts_data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_ts_data.sort_values(['product_code'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_product_data = prepared_product_data.set_index(\n",
    "            ['product_code', 'invoice_date']).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_product_data.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_codes =  [\"TBHBK\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_list_tasks = []\n",
    "for pro_code in product_codes:\n",
    "        product_list_tasks = product_list_tasks + [(pro_code)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TBHBK']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_list_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_product_data = pd.read_csv(\"D:\\\\Projects\\\\JTI\\\\Romania\\\\output\\\\feature_data\\\\feature_data_spark.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_product_data.invoice_date = pd.to_datetime(prepared_product_data.invoice_date)\n",
    "prepared_product_data = prepared_product_data.drop(columns=[\"agent_code\"])\n",
    "prepared_product_data.pos_code = prepared_product_data.pos_code.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>invoice_date</th>\n",
       "      <th>pos_code</th>\n",
       "      <th>product_code</th>\n",
       "      <th>promo_id</th>\n",
       "      <th>shipments</th>\n",
       "      <th>product_cat</th>\n",
       "      <th>is_first_nonzero_sale_date</th>\n",
       "      <th>is_last_sale_date</th>\n",
       "      <th>nonzero_Shipments_1</th>\n",
       "      <th>nonzero_Shipments_2</th>\n",
       "      <th>...</th>\n",
       "      <th>Chg_Shipments_1_from_mm_6</th>\n",
       "      <th>Chg_pct_Shipments_1_from_Shipments_2</th>\n",
       "      <th>Chg_Stock_1_from_ma_Stock_3</th>\n",
       "      <th>Chg_Stock_1_from_ma_Stock_6</th>\n",
       "      <th>Chg_pct_Stock_1_from_Stock_2</th>\n",
       "      <th>Chg_nonzero_Shipments_1_from_ma_nonzero_3</th>\n",
       "      <th>Chg_nonzero_Shipments_1_from_ma_nonzero_6</th>\n",
       "      <th>Chg_nonzero_Shipments_1_from_mm_nonzero_3</th>\n",
       "      <th>Chg_nonzero_Shipments_1_from_mm_nonzero_6</th>\n",
       "      <th>Chg_pct_nonzero_Shipments_1_from_nonzero_Shipments_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-06</td>\n",
       "      <td>10067501</td>\n",
       "      <td>TBHBKH</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>large</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-13</td>\n",
       "      <td>10067501</td>\n",
       "      <td>TBHBKH</td>\n",
       "      <td>149</td>\n",
       "      <td>0.6</td>\n",
       "      <td>large</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>666.066667</td>\n",
       "      <td>832.583333</td>\n",
       "      <td>-1.0001</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-20</td>\n",
       "      <td>10067501</td>\n",
       "      <td>TBHBKH</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>large</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>333.033333</td>\n",
       "      <td>666.066667</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-27</td>\n",
       "      <td>10067501</td>\n",
       "      <td>TBHBKH</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>large</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>499.966667</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-02-03</td>\n",
       "      <td>10067501</td>\n",
       "      <td>TBHBKH</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>large</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>333.366667</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  invoice_date  pos_code product_code  promo_id  shipments product_cat  \\\n",
       "0   2016-01-06  10067501       TBHBKH         0        0.5       large   \n",
       "1   2016-01-13  10067501       TBHBKH       149        0.6       large   \n",
       "2   2016-01-20  10067501       TBHBKH         0        0.0       large   \n",
       "3   2016-01-27  10067501       TBHBKH         0        0.0       large   \n",
       "4   2016-02-03  10067501       TBHBKH         0        0.0       large   \n",
       "\n",
       "   is_first_nonzero_sale_date  is_last_sale_date  nonzero_Shipments_1  \\\n",
       "0                        True              False                  0.0   \n",
       "1                       False              False                  0.5   \n",
       "2                       False              False                  0.6   \n",
       "3                       False              False                  0.6   \n",
       "4                       False              False                  0.6   \n",
       "\n",
       "   nonzero_Shipments_2  ...  Chg_Shipments_1_from_mm_6  \\\n",
       "0                  0.0  ...                        0.0   \n",
       "1                  0.0  ...                        0.5   \n",
       "2                  0.5  ...                        0.6   \n",
       "3                  0.5  ...                        0.0   \n",
       "4                  0.5  ...                        0.0   \n",
       "\n",
       "   Chg_pct_Shipments_1_from_Shipments_2  Chg_Stock_1_from_ma_Stock_3  \\\n",
       "0                                -999.0                     0.000000   \n",
       "1                                   5.0                   666.066667   \n",
       "2                                   0.2                   333.033333   \n",
       "3                                  -1.0                     0.333333   \n",
       "4                                -999.0                     0.166667   \n",
       "\n",
       "   Chg_Stock_1_from_ma_Stock_6  Chg_pct_Stock_1_from_Stock_2  \\\n",
       "0                     0.000000                       -0.0000   \n",
       "1                   832.583333                       -1.0001   \n",
       "2                   666.066667                        0.0000   \n",
       "3                   499.966667                        5.0000   \n",
       "4                   333.366667                        0.0000   \n",
       "\n",
       "   Chg_nonzero_Shipments_1_from_ma_nonzero_3  \\\n",
       "0                                   0.000000   \n",
       "1                                   0.333333   \n",
       "2                                   0.233333   \n",
       "3                                   0.233333   \n",
       "4                                   0.233333   \n",
       "\n",
       "   Chg_nonzero_Shipments_1_from_ma_nonzero_6  \\\n",
       "0                                   0.000000   \n",
       "1                                   0.416667   \n",
       "2                                   0.416667   \n",
       "3                                   0.416667   \n",
       "4                                   0.416667   \n",
       "\n",
       "   Chg_nonzero_Shipments_1_from_mm_nonzero_3  \\\n",
       "0                                        0.0   \n",
       "1                                        0.5   \n",
       "2                                        0.1   \n",
       "3                                        0.1   \n",
       "4                                        0.1   \n",
       "\n",
       "   Chg_nonzero_Shipments_1_from_mm_nonzero_6  \\\n",
       "0                                        0.0   \n",
       "1                                        0.5   \n",
       "2                                        0.6   \n",
       "3                                        0.6   \n",
       "4                                        0.6   \n",
       "\n",
       "   Chg_pct_nonzero_Shipments_1_from_nonzero_Shipments_2  \n",
       "0                                             -999.0     \n",
       "1                                                5.0     \n",
       "2                                                0.2     \n",
       "3                                                0.2     \n",
       "4                                                0.2     \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepared_product_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_product_df = prepared_product_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dataframes = [(\"shipment_data\", shipment_data_df),\n",
    "                   (\"future_visit_plan_df\", future_visit_plan_df),\n",
    "                   (\"prepared_product_data\", prepared_product_df),\n",
    "                   (\"bank_holidays\", bank_holidays_df),\n",
    "                   (\"shipment_split_df\", shipment_split_df),\n",
    "                   (\"sr_loading_df\", sr_loading_df),\n",
    "                   (\"sr_unloading_df\", sr_unloading_df)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "broadcasted_df = sc.broadcast(list_dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_list_tasks_RDD = sc.parallelize(product_list_tasks, numSlices=len(product_list_tasks))\n",
    "product_list_result = product_list_tasks_RDD.map(lambda pro_code: high_level_model_handler(pro_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_list_result.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High Level Model Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "code_folding": [
     34,
     50,
     61,
     73,
     80,
     124,
     126,
     145,
     156,
     162
    ]
   },
   "outputs": [],
   "source": [
    "val_start_date = '2018-01-01'\n",
    "test_start_date = '2018-07-01'\n",
    "run_phase = 'test'\n",
    "# run_phase = 'val'\n",
    "\n",
    "high_std_thr = 0.95\n",
    "low_density_thr = 0.10\n",
    "low_vp_thr = 0.75\n",
    "high_sparsity_thr = 0.10\n",
    "\n",
    "regression_mode = 'ensemble'\n",
    "\n",
    "quantity_lower_limit = 0.5\n",
    "very_high_oos_threshold = 10\n",
    "loading_size_thr = 5\n",
    "\n",
    "\n",
    "clf_drop_cols = ['VisitPlanWeek', 'week', 'is_odd_month', 'DayNumberOfWeek',\n",
    "                 'quarter', 'year', 'year_of_engagement',\n",
    "                 'shipments_1', 'shipments_2', 'shipments_3', 'shipments_4', 'shipments_5', 'shipments_6',\n",
    "                 'nonzero_Shipments_6', 'nonzero_Shipments_5', 'nonzero_Shipments_4',\n",
    "                 'nonzero_Shipments_3', 'nonzero_Shipments_2',\n",
    "                 'days_since_last_sale_3', 'days_since_last_sale_2', 'days_since_last_sale_1',\n",
    "                 'days_since_last_nonzero_sale_3', 'days_since_last_nonzero_sale_2', 'days_since_last_nonzero_sale_1',\n",
    "                 'mm_3', 'Chg_Shipments_1_from_mm_3',\n",
    "                 'mm_nonzero_3', 'Chg_nonzero_Shipments_1_from_mm_nonzero_3',\n",
    "                 'stock_6', 'stock_5', 'stock_4',\n",
    "                 'credit_request_type',\n",
    "                 # 'is_bad_weather_last_three', 'is_bad_weather_last_six',\n",
    "                 # below two lines are features that shouldn't ever be included in model #\n",
    "                 'is_last_sale_date', 'isMissedPlan', 'stock', 'promo_id',\n",
    "                 'isVisitPlan', 'is_zero_sale', 'is_first_nonzero_sale_date']\n",
    "\n",
    "keep_in_reg_result = ['invoice_date', 'known_shipment', 'predicted_loading',\n",
    "                      'product_cat', 'pos_code',\n",
    "                      'VisitPlanWeek', 'month', 'year', 'week', 'days_from_easter',\n",
    "                      'pre_nonreplacement_holiday',\n",
    "                      'pred_is_nonzero_shipments',\n",
    "                      'double_sell', 'triple_sell', 'credit_request_type',\n",
    "                      'double_sell_1', 'triple_sell_1',\n",
    "                      'days_since_price_chg_ann', 'days_from_price_chg',\n",
    "                      'isVisitPlan_1']\n",
    "use_drop_reg_cols_instead_of_keep = False\n",
    "reg_keep_cols_1 = ['pos_code', 'week', 'month',\n",
    "                   'nonzero_Shipments_1', 'nonzero_Shipments_2', 'nonzero_Shipments_3',\n",
    "                   'nonzero_Shipments_4', 'nonzero_Shipments_5', 'nonzero_Shipments_6',\n",
    "                   'days_since_last_nonzero_sale', 'ma_nonzero_interval_3',\n",
    "                   'is_first_plan_date', 'days_since_first_nonzero_sale',\n",
    "                   'double_sell', 'triple_sell', 'credit_request_coeff',\n",
    "                   'double_sell_1', 'triple_sell_1',\n",
    "                   'days_since_price_chg_ann', 'return_1',\n",
    "                   'pre_nonreplacement_holiday', 'isVisitPlan_1',\n",
    "                   'is_zero_sale_1', 'num_consecutive_zero_sales',\n",
    "                   'ma_3', 'ma_6',\n",
    "                   'ma_nonzero_3', 'ma_nonzero_6']\n",
    "reg_keep_cols_2 = ['DayNumberOfWeek', 'pos_code',\n",
    "                   'pre_nonreplacement_holiday', 'month',\n",
    "                   'days_since_price_chg_ann',\n",
    "                   'triple_sell', 'double_sell',\n",
    "                   'credit_request_coeff', 'days_from_easter']\n",
    "                   # 'is_bad_weather_last_three', 'is_bad_weather_last_six']\n",
    "\n",
    "reg_drop_cols = ['shipments_6', 'shipments_5', 'shipments_4',  #'shipments_3', 'shipments_2',#'shipments_1',\n",
    "                 'Chg_pct_Shipments_1_from_Shipments_2',\n",
    "                 'ma_6', 'ma_3',\n",
    "                 # 'Chg_Shipments_1_from_ma_6', 'Chg_Shipments_1_from_ma_3',\n",
    "                 'mm_6', 'mm_3',\n",
    "                 'Chg_Shipments_1_from_mm_6', 'Chg_Shipments_1_from_mm_3',\n",
    "                 'days_since_first_sale',  #'is_first_plan_date',\n",
    "                 # 'ma_interval_3',\n",
    "                 'days_since_last_sale_3', 'days_since_last_sale_2',\n",
    "                 'days_since_last_sale_1', 'days_since_last_sale',\n",
    "                 'double_sell_lead_1',  # 'triple_sell_lead_1',\n",
    "                 # 'double_sell_2', 'double_sell_1', 'double_sell',\n",
    "                 # 'triple_sell_2', 'triple_sell_1', 'triple_sell',\n",
    "                 'credit_request_type',  # 'days_from_easter', 'credit_request_coeff',\n",
    "                 'isVisitPlan_2',  #'isVisitPlan_1', 'return_1',\n",
    "                 'days_from_price_chg', # 'days_since_price_chg_ann', 'PricelistChg_1',\n",
    "                 'num_consecutive_nonzero_sales',  #'num_consecutive_zero_sales',\n",
    "                 # 'nonzero_Shipments_6', 'nonzero_Shipments_5', 'nonzero_Shipments_4',\n",
    "                 # 'nonzero_Shipments_3', 'nonzero_Shipments_2', 'nonzero_Shipments_1',\n",
    "                 'Chg_pct_nonzero_Shipments_1_from_nonzero_Shipments_2',\n",
    "                 # 'ma_nonzero_6', 'ma_nonzero_3',\n",
    "                 'Chg_nonzero_Shipments_1_from_ma_nonzero_6',  #'Chg_nonzero_Shipments_1_from_ma_nonzero_3',\n",
    "                 'mm_nonzero_6', 'mm_nonzero_3',\n",
    "                 'Chg_nonzero_Shipments_1_from_mm_nonzero_6', 'Chg_nonzero_Shipments_1_from_mm_nonzero_3',\n",
    "                 'isMissedPlan_2',  #'isMissedPlan_1',\n",
    "                 'stock_6', 'stock_5', 'stock_4', 'stock_3',  #'stock_2', 'stock_1',\n",
    "                 'Chg_pct_Stock_1_from_Stock_2',\n",
    "                 'ma_Stock_6', 'ma_Stock_3',\n",
    "                 'Chg_Stock_1_from_ma_Stock_6', 'Chg_Stock_1_from_ma_Stock_3',\n",
    "                 # 'days_since_first_nonzero_sale',\n",
    "                 # 'ma_nonzero_interval_3',\n",
    "                 'days_since_last_nonzero_sale_3', 'days_since_last_nonzero_sale_2',\n",
    "                 # 'days_since_last_nonzero_sale_1', 'days_since_last_nonzero_sale',\n",
    "                 'week', 'weekday_holiday', 'is_odd_month',\n",
    "                 # 'month', 'quarter', 'year', 'year_of_engagement', 'DayNumberOfWeek',\n",
    "                 'promo_id', 'is_zero_sale_3', 'is_zero_sale_2',  #'is_zero_sale_1',\n",
    "                 'product_cat',  #'pre_nonreplacement_holiday',\n",
    "                 # 'is_bad_weather_last_three', 'is_bad_weather_last_six',\n",
    "                 # below two lines are features that shouldn't ever be included in model #\n",
    "                 'stock', 'isMissedPlan', 'is_last_sale_date', 'is_zero_sale',\n",
    "                 'is_first_nonzero_sale_date', 'isVisitPlan']\n",
    "\n",
    "# Summer event hosting markets related params\n",
    "ct_event_agents = ['1810', '1811', '1815', '1816', '1822',\n",
    "                   '1823', '1825', '1826', '1828', '1829']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# aggregates predicted shipments at date, agent level for a product\n",
    "# and applies dynamic buffering to get loading prediction\n",
    "def high_level_post_model_aggregator(product_date_agent_forecasts_data: pd.DataFrame,\n",
    "                                     train_data_with_agent: pd.DataFrame,\n",
    "                                     past_test_data_with_agent: pd.DataFrame,\n",
    "                                     inflated_demand_marker_data: pd.DataFrame,\n",
    "                                     agg_buffer_pct: float,\n",
    "                                     force_loading_upper_limit: bool) -> pd.DataFrame:\n",
    "    current_date = product_date_agent_forecasts_data['visit_date'].iloc[0]\n",
    "    current_agent = product_date_agent_forecasts_data['agent_code'].iloc[0]\n",
    "    # print(current_date, current_agent)\n",
    "\n",
    "    train_data_cols = ['visit_date', 'agent_code', 'pos_code', 'shipments']\n",
    "    poswise_forecast = product_date_agent_forecasts_data.copy()\n",
    "    current_pos_list = poswise_forecast['pos_code']\n",
    "    train_df = train_data_with_agent.loc[\n",
    "        train_data_with_agent['pos_code'].isin(current_pos_list), train_data_cols]\n",
    "    # The pos-set wise data for days in latest week can paint an incomplete aggregate picture.\n",
    "    # Thus pos-set wise data should only be taken till last week to current date\n",
    "    past_test_data_with_agent = past_test_data_with_agent.loc[\n",
    "        (past_test_data_with_agent['year'] < current_date.year)\n",
    "        | (past_test_data_with_agent['week'] < current_date.weekofyear)]\n",
    "    if len(past_test_data_with_agent) > 0:\n",
    "        past_test_df = past_test_data_with_agent.loc[\n",
    "            past_test_data_with_agent['pos_code'].isin(current_pos_list), train_data_cols]\n",
    "        past_test_df = past_test_df.loc[past_test_df['visit_date'] < current_date]\n",
    "        train_df = pd.concat([train_df, past_test_df], ignore_index=True)\n",
    "    train_agent_df = train_data_with_agent.loc[\n",
    "        train_data_with_agent['agent_code'] == current_agent, train_data_cols]\n",
    "    if len(past_test_data_with_agent) > 0:\n",
    "        past_test_agent_df = past_test_data_with_agent.loc[\n",
    "            past_test_data_with_agent['agent_code'] == current_agent, train_data_cols]\n",
    "        past_test_agent_df = past_test_agent_df.loc[past_test_agent_df['visit_date'] < current_date]\n",
    "        train_agent_df = pd.concat([train_agent_df, past_test_agent_df], ignore_index=True)\n",
    "    new_pos_list = current_pos_list[~current_pos_list.isin(train_df['pos_code'])]\n",
    "    # print('new pos count:', glen(new_pos_list))\n",
    "    new_pos_count = len(new_pos_list)\n",
    "    train_pos_count = len(current_pos_list) - new_pos_count\n",
    "    current_pos_count = len(current_pos_list)\n",
    "    if train_pos_count != 0:\n",
    "        extrapolation_factor: float = current_pos_count/train_pos_count\n",
    "    else:\n",
    "        extrapolation_factor: float = np.nan\n",
    "\n",
    "    # Agg the results\n",
    "    agg_forecast = poswise_forecast.groupby(['visit_date']).agg(\n",
    "        {'predicted_loading': 'sum',\n",
    "         'known_shipment': 'sum',\n",
    "         'orig_pred_load': 'sum',\n",
    "         'pred_is_nonzero_shipments': 'mean',\n",
    "         'pre_nonreplacement_holiday': 'mean',\n",
    "         'VisitPlanWeek': 'first',\n",
    "         'week': 'first',\n",
    "         'month': 'first',\n",
    "         'year': 'first',\n",
    "         'credit_request_type': 'first',\n",
    "         'days_from_easter': 'first',\n",
    "         'days_since_price_chg_ann': 'first',\n",
    "         'days_from_price_chg': 'first',\n",
    "         'agent_code': 'first',\n",
    "         'product_cat': 'first'})\n",
    "    assert len(agg_forecast) == 1\n",
    "    agg_forecast['original_predicted_loading'] = agg_forecast['predicted_loading']\n",
    "    agg_forecast['pred_is_nonzero_shipments'] = \\\n",
    "        agg_forecast['pred_is_nonzero_shipments'].astype('float')\n",
    "    agg_forecast['pred_zero_pct'] = 1 - agg_forecast['pred_is_nonzero_shipments']\n",
    "    agg_forecast = agg_forecast.drop(columns=['pred_is_nonzero_shipments'])\n",
    "    agg_forecast['pre_nonreplacement_holiday'] = \\\n",
    "        agg_forecast['pre_nonreplacement_holiday'].astype('float')\n",
    "    agg_forecast['NationalDoubleSell'] = inflated_demand_marker_data['NationalDoubleSell']\n",
    "    agg_forecast['NationalTripleSell'] = inflated_demand_marker_data['NationalTripleSell']\n",
    "    agg_forecast['curr_pos_count'] = [current_pos_count]\n",
    "    agg_forecast['new_pos_count'] = [new_pos_count]\n",
    "    assert len(agg_forecast) == 1\n",
    "\n",
    "    nan_count = 0\n",
    "    if train_pos_count == 0:\n",
    "        limit_pos_set_max_sale = np.nan\n",
    "        limit_pos_set_peak_sale = np.nan\n",
    "        limit_pos_set_very_high_sale = np.nan\n",
    "        limit_pos_set_high_sale = np.nan\n",
    "        limit_pos_set_median_sale = np.nan\n",
    "        train_shipments_mean = np.nan\n",
    "        train_shipments_min = np.nan\n",
    "        nan_count += 1\n",
    "    else:\n",
    "        # prepare\n",
    "        shipments_train_sample = train_df.groupby(\n",
    "            [pd.Grouper(key='visit_date', freq='W'), 'pos_code']).agg({'shipments': 'mean'})\n",
    "        shipments_train_sample = shipments_train_sample.reset_index().groupby(\n",
    "            'visit_date').agg({'shipments': 'sum', 'pos_code': 'count'})\n",
    "        shipments_train_sample.columns = ['shipments_train_sum', 'pos_code_count']\n",
    "        limit_pos_set_max_sale = (shipments_train_sample['shipments_train_sum'].quantile(1)\n",
    "                                  * extrapolation_factor)\n",
    "        limit_pos_set_peak_sale = (shipments_train_sample['shipments_train_sum'].quantile(0.999)\n",
    "                                   * extrapolation_factor)\n",
    "        limit_pos_set_very_high_sale = (shipments_train_sample['shipments_train_sum'].quantile(0.99)\n",
    "                                        * extrapolation_factor)\n",
    "        limit_pos_set_high_sale = (shipments_train_sample['shipments_train_sum'].quantile(0.98)\n",
    "                                   * extrapolation_factor)\n",
    "        limit_pos_set_median_sale = (shipments_train_sample['shipments_train_sum'].median()\n",
    "                                     * extrapolation_factor)\n",
    "\n",
    "        shipments_train_sample = shipments_train_sample.last('6M')\n",
    "        train_shipments_mean = shipments_train_sample['shipments_train_sum'].mean()\n",
    "        train_shipments_min = shipments_train_sample['shipments_train_sum'].min()\n",
    "    if len(train_agent_df) > 0:\n",
    "        train_agent_shipments = train_agent_df.groupby(\n",
    "            'visit_date').agg({'shipments': 'sum'})\n",
    "        train_agent_shipments.columns = ['agent_shipments_train_sum']\n",
    "        limit_agent_max_sale = train_agent_shipments[\n",
    "            'agent_shipments_train_sum'].quantile(1)\n",
    "        limit_agent_peak_sale = train_agent_shipments[\n",
    "            'agent_shipments_train_sum'].quantile(0.999)\n",
    "        limit_agent_very_high_sale = train_agent_shipments[\n",
    "            'agent_shipments_train_sum'].quantile(0.99)\n",
    "        limit_agent_high_sale = train_agent_shipments[\n",
    "            'agent_shipments_train_sum'].quantile(0.98)\n",
    "        limit_agent_median_sale = train_agent_shipments[\n",
    "            'agent_shipments_train_sum'].median()\n",
    "    else:\n",
    "        limit_agent_max_sale = np.nan\n",
    "        limit_agent_peak_sale = np.nan\n",
    "        limit_agent_very_high_sale = np.nan\n",
    "        limit_agent_high_sale = np.nan\n",
    "        limit_agent_median_sale = np.nan\n",
    "        nan_count += 1\n",
    "    if nan_count < 2:\n",
    "        limit_max_sale = np.nanmax([limit_pos_set_max_sale, limit_agent_max_sale])\n",
    "        limit_peak_sale = np.nanmax([limit_pos_set_peak_sale, limit_agent_peak_sale])\n",
    "        limit_very_high_sale = np.nanmax([limit_pos_set_very_high_sale, limit_agent_very_high_sale])\n",
    "        limit_high_sale = np.nanmax([limit_pos_set_high_sale, limit_agent_high_sale])\n",
    "    else:\n",
    "        # case where both agent and pos set is entirely new\n",
    "        limit_max_sale = np.nan\n",
    "        limit_peak_sale = np.nan\n",
    "        limit_very_high_sale = np.nan\n",
    "        limit_high_sale = np.nan\n",
    "    if limit_pos_set_median_sale != np.nan and limit_agent_median_sale != np.nan:\n",
    "        limit_median_sale = np.nanmean([limit_pos_set_median_sale, limit_agent_median_sale])\n",
    "    elif limit_pos_set_median_sale != np.nan:\n",
    "        limit_median_sale = limit_pos_set_median_sale\n",
    "    elif limit_agent_median_sale != np.nan:\n",
    "        limit_median_sale = limit_agent_median_sale\n",
    "    else:\n",
    "        limit_median_sale = quantity_lower_limit\n",
    "    agg_forecast['train_shipments_mean'] = [train_shipments_mean]\n",
    "    agg_forecast['train_shipments_min'] = [train_shipments_min]\n",
    "    agg_forecast['limit_max_sale'] = [limit_max_sale]\n",
    "    agg_forecast['limit_peak_sale'] = [limit_peak_sale]\n",
    "    agg_forecast['limit_very_high_sale'] = [limit_very_high_sale]\n",
    "    agg_forecast['limit_high_sale'] = [limit_high_sale]\n",
    "    agg_forecast['limit_median_sale'] = [limit_median_sale]\n",
    "\n",
    "    # agg loading buffer\n",
    "    def _agg_loading_levels_list(predicted_loading):\n",
    "        return [predicted_loading * (1 + agg_buffer_pct),\n",
    "                predicted_loading + agg_loading_buffer]\n",
    "\n",
    "    agg_forecast['nominal_agg_buffer'] = [train_shipments_mean*agg_buffer_pct]\n",
    "    train_shipments_mean = train_shipments_mean*extrapolation_factor\n",
    "    agg_loading_buffer = train_shipments_mean*agg_buffer_pct\n",
    "    agg_forecast['used_agg_buffer'] = [agg_loading_buffer]\n",
    "    # print(agg_loading_buffer)\n",
    "    agg_forecast['predicted_loading'] = \\\n",
    "        agg_forecast['predicted_loading'].apply(lambda x:\n",
    "                                                np.nanmean(_agg_loading_levels_list(x)))\n",
    "\n",
    "    # Upper limit\n",
    "    if force_loading_upper_limit:\n",
    "        if (agg_forecast['NationalTripleSell'].iloc[0]\n",
    "                or (agg_forecast['credit_request_type'].iloc[0] == 'TRIPLE')):\n",
    "            agg_forecast['predicted_loading'] = [np.nanmin(\n",
    "                [agg_forecast['predicted_loading'].iloc[0], limit_max_sale])]\n",
    "        elif (agg_forecast['NationalDoubleSell'].iloc[0]\n",
    "              or (agg_forecast['credit_request_type'].iloc[0] == 'DOUBLE')\n",
    "              or (agg_forecast['days_from_easter'].iloc[0] >= 0)\n",
    "              or agg_forecast['pre_nonreplacement_holiday'].iloc[0] > 0):\n",
    "            agg_forecast['predicted_loading'] = [np.nanmin(\n",
    "                [agg_forecast['predicted_loading'].iloc[0], limit_peak_sale])]\n",
    "        elif agg_forecast['predicted_loading'].iloc[0] <= loading_size_thr:\n",
    "            agg_forecast['predicted_loading'] = [np.nanmin(\n",
    "                [agg_forecast['predicted_loading'].iloc[0], limit_very_high_sale])]\n",
    "        else:\n",
    "            agg_forecast['predicted_loading'] = [np.nanmin(\n",
    "                [agg_forecast['predicted_loading'].iloc[0], limit_high_sale])]\n",
    "\n",
    "    # if ((agg_forecast.index.month[0] == 12 and agg_forecast.index.day[0] < 20)\n",
    "    #         or (agg_forecast.index.month[0] < 12)):\n",
    "    agg_forecast['predicted_loading'] = \\\n",
    "        agg_forecast['predicted_loading'].clip(limit_median_sale, None)\n",
    "\n",
    "    # Finalize\n",
    "    agg_forecast['predicted_loading'] = \\\n",
    "        np.round(agg_forecast['predicted_loading'] + np.nanmin([2, train_shipments_min * 0.2]), 1)\n",
    "    if agg_forecast['predicted_loading'].iloc[0] <= quantity_lower_limit:\n",
    "        agg_forecast['predicted_loading'] = quantity_lower_limit + 0.1\n",
    "    else:\n",
    "        agg_forecast['predicted_loading'] = np.round(agg_forecast['predicted_loading'] + 0.5, 1)\n",
    "\n",
    "    return agg_forecast\n",
    "\n",
    "\n",
    "def high_level_post_agg_comparison_prep(agg_result_data: pd.DataFrame,\n",
    "                                        product_code: str,\n",
    "                                        shipment_data: pd.DataFrame,\n",
    "                                        shipment_split_data: pd.DataFrame,\n",
    "                                        sr_loading_data: pd.DataFrame,\n",
    "                                        sr_unloading_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    forecast_comparison_df = agg_result_data.reset_index().set_index(\n",
    "        ['visit_date', 'agent_code', 'product_code'])\n",
    "\n",
    "    # Add shipment split data: plan compliant & ER\n",
    "    forecast_comparison_df = forecast_comparison_df.merge(\n",
    "        shipment_split_data,\n",
    "        how='left',\n",
    "        on=['visit_date', 'agent_code', 'product_code'])\n",
    "    forecast_comparison_df.loc[:, ['non_vp_shipment',\n",
    "                                   'vp_shipment']] = \\\n",
    "        forecast_comparison_df.loc[:, ['non_vp_shipment', 'vp_shipment']].fillna(0)\n",
    "\n",
    "    # Add actual shipment for comparison with predicted loading\n",
    "    shipment_compare = shipment_data[shipment_data['product_code'] == product_code]\n",
    "    shipment_compare = shipment_compare.groupby(['invoice_date',\n",
    "                                                 'agent_code',\n",
    "                                                 'product_code'])[['quantity']].sum()\n",
    "    shipment_compare.reset_index(inplace=True)\n",
    "    shipment_compare.rename(columns={'invoice_date': 'visit_date'}, inplace=True)\n",
    "    shipment_compare.set_index(['visit_date', 'agent_code', 'product_code'], inplace=True)\n",
    "    forecast_comparison_df['actual_shipment'] = shipment_compare['quantity'].round(1)\n",
    "    # For non-ER runs\n",
    "    # forecast_comparison_df['total_shipment'] = shipment_compare['quantity'].round(1)\n",
    "    # forecast_comparison_df['actual_shipment'] = forecast_comparison_df['vp_shipment'].round(1)\n",
    "\n",
    "    # add actual loading data\n",
    "    forecast_comparison_df['sr_loading'] = sr_loading_data['quantity']\n",
    "    forecast_comparison_df['sr_unloading'] = sr_unloading_data['quantity']\n",
    "\n",
    "    # if all loaded quantities are unloaded, loading won't be null but shipment wil be\n",
    "    forecast_comparison_df.loc[\n",
    "        forecast_comparison_df['sr_loading'].notna(), 'actual_shipment'] = forecast_comparison_df.loc[\n",
    "        forecast_comparison_df['sr_loading'].notna(), 'actual_shipment'].fillna(0)\n",
    "\n",
    "    # performance comparison\n",
    "    residual = forecast_comparison_df['predicted_loading'] - forecast_comparison_df['actual_shipment']\n",
    "    forecast_comparison_df['asl_unloading'] = residual.apply(lambda x: x if x > 0 else 0)\n",
    "    forecast_comparison_df['OOS_amt_indicative'] = residual.apply(lambda x: -x if x < 0 else 0)\n",
    "    forecast_comparison_df['is_OOS'] = residual.apply(lambda x: 1 if x <= 0 else 0)\n",
    "    jti_residual = forecast_comparison_df['sr_loading'] - forecast_comparison_df['actual_shipment']\n",
    "    forecast_comparison_df['jti_unloading'] = jti_residual.apply(lambda x: x if x > 0 else 0)\n",
    "    forecast_comparison_df['jti_OOS'] = jti_residual.apply(lambda x: 1 if x <= 0 else 0)\n",
    "\n",
    "    return forecast_comparison_df\n",
    "\n",
    "\n",
    "def get_forecast_performance_figures(forecast_comparison_data: pd.DataFrame,) -> tuple:\n",
    "    forecast_comparison_df = forecast_comparison_data[\n",
    "        forecast_comparison_data['sr_loading'].notna()]\n",
    "    pred_unloading = (forecast_comparison_df['asl_unloading'].sum()\n",
    "                      / forecast_comparison_df['predicted_loading'].sum())*100\n",
    "    pred_oos = forecast_comparison_df['is_OOS'].mean()*100\n",
    "    jti_unloading = (forecast_comparison_df['jti_unloading'].sum()\n",
    "                     / forecast_comparison_df['sr_loading'].sum())*100\n",
    "    jti_oos = forecast_comparison_df['jti_OOS'].mean()*100\n",
    "\n",
    "    return pred_unloading, pred_oos, jti_unloading, jti_oos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_regressor(x_trn: pd.DataFrame,\n",
    "                 y_trn: np.ndarray,\n",
    "                 x_val: pd.DataFrame,\n",
    "                 y_val: np.ndarray) -> tuple:\n",
    "    x_trn, x_val = x_trn.copy(), x_val.copy()\n",
    "    y_trn, y_val = y_trn.copy(), y_val.copy()\n",
    "    model = RandomForestRegressor(n_estimators=400, min_samples_leaf=3,\n",
    "                                  n_jobs=-1, random_state=7)\n",
    "    _ = model.fit(x_trn, y_trn)\n",
    "\n",
    "    training_score = model.score(x_trn, y_trn)\n",
    "    validation_score = model.score(x_val, y_val)\n",
    "\n",
    "    return model, training_score, validation_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_classifier(x_trn: pd.DataFrame,\n",
    "                  y_trn: np.ndarray,\n",
    "                  x_val: pd.DataFrame,\n",
    "                  y_val: np.ndarray) -> tuple:\n",
    "    x_trn, x_val = x_trn.copy(), x_val.copy()\n",
    "    y_trn, y_val = y_trn.copy(), y_val.copy()\n",
    "\n",
    "    model = RandomForestClassifier(n_estimators=400, min_samples_leaf=16,\n",
    "                                   class_weight='balanced',\n",
    "                                   n_jobs=1, random_state=7)\n",
    "    _ = model.fit(x_trn, y_trn)\n",
    "\n",
    "    training_score = model.score(x_trn, y_trn)\n",
    "    validation_score = model.score(x_val, y_val)\n",
    "\n",
    "    clf_report = classification_report(y_val, model.predict(x_val))\n",
    "    ck_score = cohen_kappa_score(y_val, model.predict(x_val))\n",
    "\n",
    "    return model, training_score, validation_score, clf_report, ck_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High Level Model Handler Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_date_split_model_forecast(product_data: pd.DataFrame,\n",
    "                                     inflated_demand_marker_data: pd.DataFrame,\n",
    "                                     present_agent_map_data: pd.DataFrame,\n",
    "                                     past_agent_map_data: pd.DataFrame,\n",
    "                                     pred_date: pd.Timestamp,\n",
    "                                     remove_first_month_from_training: bool,\n",
    "                                     y_col: str,\n",
    "                                     model_period: str,\n",
    "                                     hyperparams_dict: dict,\n",
    "                                     product_code: str,\n",
    "                                     so_code: str,\n",
    "                                     model_pickle_path: str,\n",
    "                                     run_log_path: str,\n",
    "                                     int_result_dump_path: str):\n",
    "    period_column_switcher: dict = {\n",
    "        'annual': 'year',\n",
    "        'monthly': 'month',\n",
    "        'weekly': 'week'}\n",
    "    period_column = period_column_switcher[model_period]\n",
    "    period_designator = eval('pred_date.{}'.format(period_column))\n",
    "    # print(model_period)\n",
    "\n",
    "    # split data into train & test segments\n",
    "    _train_test_split = train_test_split(\n",
    "        product_data, pred_date, remove_first_month_from_training, model_period)\n",
    "    if _train_test_split is not None:\n",
    "        df_train, df_test, df_leftover, extra_test_features_introduced = _train_test_split\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    # check for condition for re-training period model\n",
    "    period_data =product_data.loc[product_data['year'] == pred_date.year]\n",
    "    period_data = period_data.loc[period_data[period_column] == period_designator]\n",
    "    dates_in_period_sorted = period_data.sort_index().index.unique()\n",
    "    is_pred_date_first_in_period: bool = (dates_in_period_sorted[0] == pred_date)\n",
    "\n",
    "    # model data & get forecast\n",
    "    df_test_after_clf, forecast_result = model(\n",
    "        df_train,\n",
    "        df_test,\n",
    "        y_col,\n",
    "        is_pred_date_first_in_period,\n",
    "        extra_test_features_introduced,\n",
    "        model_period,\n",
    "        product_code,\n",
    "        so_code,\n",
    "        model_pickle_path,\n",
    "        run_log_path\n",
    "    )\n",
    "\n",
    "    # post model forecast data prep\n",
    "    forecast_result =forecast_result[\n",
    "        keep_in_reg_result + extra_test_features_introduced]\n",
    "    forecast_result = forecast_result.rename(columns={'invoice_date': 'visit_date'})\n",
    "    forecast_result = forecast_result.merge(\n",
    "        present_agent_map_data,\n",
    "        how='left',\n",
    "        on=['visit_date', 'pos_code'])\n",
    "    forecast_result['orig_pred_load'] = forecast_result['predicted_loading']\n",
    "    # print(forecast_result.head())\n",
    "    train_data_with_agent = df_train.reset_index()\n",
    "    train_data_with_agent = train_data_with_agent.merge(past_agent_map_data,\n",
    "                                                        how='left',\n",
    "                                                        on=['invoice_date', 'pos_code'])\n",
    "    train_data_with_agent.rename(columns={'invoice_date': 'visit_date'}, inplace=True)\n",
    "    # print(train_data_with_agent.head())\n",
    "    past_data_not_in_train_with_agent = df_leftover.reset_index()\n",
    "    past_data_not_in_train_with_agent = past_data_not_in_train_with_agent.merge(\n",
    "        past_agent_map_data,\n",
    "        how='left',\n",
    "        on=['invoice_date', 'pos_code'])\n",
    "    past_data_not_in_train_with_agent.rename(columns={'invoice_date': 'visit_date'}, inplace=True)\n",
    "    # print(past_data_not_in_train_with_agent)\n",
    "\n",
    "    # forecast aggregation & buffer addition\n",
    "    forecast_df =forecast_result.groupby(\n",
    "        'agent_code').apply(high_level_post_model_operations,\n",
    "                                     train_data_with_agent=train_data_with_agent,\n",
    "                                     past_test_data_with_agent=past_data_not_in_train_with_agent,\n",
    "                                     inflated_demand_marker_data=inflated_demand_marker_data,\n",
    "                                     model_type=model_period,\n",
    "                                     product_code=product_code,\n",
    "                                     hyperparams_dict=hyperparams_dict,\n",
    "                                     int_result_dump_path=int_result_dump_path,\n",
    "                                     run_log_path=run_log_path)\n",
    "    forecast_df = forecast_df.drop(columns=['agent_code'])\n",
    "\n",
    "    return forecast_result, forecast_df\n",
    "\n",
    "\n",
    "def train_test_split(product_data: pd.DataFrame,\n",
    "                     pred_date: pd.Timestamp,\n",
    "                     remove_first_month_from_training: bool,\n",
    "                     model_period = None):\n",
    "    first_month_in_data = product_data['month'].min()\n",
    "    first_year_in_data = product_data['year'].min()\n",
    "\n",
    "    if model_period == 'annual':\n",
    "        df_train =product_data.loc[product_data['year'] < pred_date.year].copy()\n",
    "    elif model_period == 'monthly':\n",
    "        df_train =product_data.loc[product_data.index\n",
    "                                                  < pd.Timestamp(year=pred_date.year,\n",
    "                                                                 month=pred_date.month,\n",
    "                                                                 day=1)].copy()\n",
    "    elif model_period == 'weekly':\n",
    "        df_train =product_data.loc[(product_data['year'] < pred_date.year)\n",
    "                                                  | (product_data['week'] < pred_date.weekofyear)].copy()\n",
    "    else:\n",
    "        df_train =product_data.loc[product_data.index < pred_date].copy()\n",
    "\n",
    "    if remove_first_month_from_training:\n",
    "        df_train = df_train.loc[\n",
    "            ~((df_train['month'] == first_month_in_data)\n",
    "              & (df_train['year'] == first_year_in_data))].copy()\n",
    "\n",
    "    df_test =product_data.loc[\n",
    "        product_data.index == pred_date]\n",
    "    df_test = df_test.loc[df_test['isVisitPlan']].copy()\n",
    "    if len(df_test) == 0:\n",
    "        return None\n",
    "    problematic_pos, df_test, extra_test_features_introduced = \\\n",
    "        _flag_problematic_pos(df_train, df_test.reset_index())\n",
    "    df_test = df_test.set_index('invoice_date')\n",
    "\n",
    "    df_leftover = product_data.loc[(product_data.index > df_train.index.max())\n",
    "                                   & (product_data.index < df_test.index.min())]\n",
    "\n",
    "    return df_train, df_test, df_leftover, extra_test_features_introduced\n",
    "\n",
    "\n",
    "def prepare_features_for_sklearn_modelling(x_train_df: pd.DataFrame,\n",
    "                                           x_test_df: pd.DataFrame) -> tuple:\n",
    "    # transform the features to a format compatible with sklearn ml models\n",
    "    categorize_train_dict = _categorize_train(x_train_df)\n",
    "    x_train = categorize_train_dict['x_train_df']\n",
    "\n",
    "    data_preproc_train_dict = _data_preproc_train(x_train, max_cat_count=0)\n",
    "    x_train = data_preproc_train_dict['x_train_df']\n",
    "\n",
    "    x_test, _ = _categorize_val(categorize_train_dict, x_test_df)\n",
    "    x_test, _ = _data_preproc_val(data_preproc_train_dict, x_test)\n",
    "    # for cases where category variable seen for first time in x_test\n",
    "    x_test = x_test.fillna(0)\n",
    "\n",
    "    return x_train, x_test\n",
    "\n",
    "\n",
    "def nonzero_shipment_classification(df_train_clf: pd.DataFrame,\n",
    "                                    df_test_clf: pd.DataFrame,\n",
    "                                    y_col: str,\n",
    "                                    clf_cols_to_remove: list,\n",
    "                                    extra_test_features_introduced: list,\n",
    "                                    product_code: str,\n",
    "                                    rerun_model: bool,\n",
    "                                    model_pickle_path: str,\n",
    "                                    run_log_path: str) -> pd.DataFrame:\n",
    "    clf_train_features_to_remove = clf_cols_to_remove\n",
    "    clf_train_features_to_remove.append(y_col)\n",
    "    # print(clf_train_features_to_remove)\n",
    "    y_train_clf = df_train_clf[y_col]\n",
    "    x_train_clf =df_train_clf.drop(columns=clf_train_features_to_remove)\n",
    "    # print(x_train_clf.head())\n",
    "    # print(x_train_clf.columns.tolist())\n",
    "    # print('no. of features:', glen(x_train.columns))\n",
    "\n",
    "    clf_test_features_to_remove = clf_train_features_to_remove.copy()\n",
    "    clf_test_features_to_remove.extend(extra_test_features_introduced)\n",
    "    # print(clf_test_features_to_remove)\n",
    "    y_test_clf = df_test_clf[y_col]\n",
    "    x_test_clf =df_test_clf.drop(columns=clf_test_features_to_remove)\n",
    "    # print(x_test_clf.head())\n",
    "    # print(x_test_clf.columns.tolist())\n",
    "\n",
    "    x_train_clf_proc, x_test_clf_proc = \\\n",
    "        prepare_features_for_sklearn_modelling(x_train_clf, x_test_clf)\n",
    "\n",
    "    if rerun_model:\n",
    "        model_clf, clf_trn_true_pct, clf_test_true_pct, \\\n",
    "            clf_trn_score, clf_test_score, clf_report, clf_ck_score = \\\n",
    "            high_level_rf_clf_modeler(x_train_clf_proc,\n",
    "                                      y_train_clf,\n",
    "                                      x_test_clf_proc,\n",
    "                                      y_test_clf)\n",
    "        dump(model_clf, model_pickle_path)\n",
    "        with open(run_log_path, 'a') as model_log:\n",
    "            model_log.write(product_code + '\\n')\n",
    "            model_log.write('clf train true percent: ' + str(clf_trn_true_pct) + '\\n')\n",
    "            model_log.write('clf test true percent: ' + str(clf_test_true_pct) + '\\n')\n",
    "            model_log.write('clf train score: ' + str(clf_trn_score) + '\\n')\n",
    "            model_log.write('clf test score: ' + str(clf_test_score) + '\\n')\n",
    "            model_log.write('clf report:\\n' + str(clf_report) + '\\n')\n",
    "            model_log.write('clf ck score: ' + str(clf_ck_score) + '\\n')\n",
    "    else:\n",
    "        model_clf = load(model_pickle_path)\n",
    "\n",
    "    is_nonzero_test = model_clf.predict(x_test_clf_proc)\n",
    "    assert len(is_nonzero_test) == len(df_test_clf)\n",
    "    df_test_clf.loc[:, 'pred_is_nonzero_shipments'] = is_nonzero_test\n",
    "    # print(df_test_clf.head())\n",
    "\n",
    "    return df_test_clf\n",
    "\n",
    "\n",
    "def drop_cols_regression(df_train_reg: pd.DataFrame,\n",
    "                         df_test_reg: pd.DataFrame,\n",
    "                         y_col: str,\n",
    "                         extra_test_features_introduced: list,\n",
    "                         rerun_model: bool,\n",
    "                         model_pickle_path: str) -> np.ndarray:\n",
    "    reg_train_features_to_remove = reg_drop_cols.copy()\n",
    "    reg_train_features_to_remove.append(y_col)\n",
    "    x_train_reg =df_train_reg.drop(\n",
    "        columns=reg_train_features_to_remove)\n",
    "    # print(x_train_reg.head())\n",
    "    # print(x_train_reg.columns)\n",
    "    y_train_reg = df_train_reg[y_col]\n",
    "\n",
    "    reg_test_features_to_remove = reg_train_features_to_remove.copy()\n",
    "    reg_test_features_to_remove.extend(extra_test_features_introduced)\n",
    "    reg_test_features_to_remove.append('pred_is_nonzero_shipments')\n",
    "    x_test_reg =df_test_reg.drop(\n",
    "        columns=reg_test_features_to_remove)\n",
    "    # print(x_test_reg.head())\n",
    "    # print(x_test_reg.columns)\n",
    "    y_test_reg = df_test_reg[y_col]\n",
    "\n",
    "    x_train_reg_proc, x_test_reg_proc = \\\n",
    "        prepare_features_for_sklearn_modelling(x_train_reg, x_test_reg)\n",
    "\n",
    "    if rerun_model:\n",
    "        model_reg, x_test_reg_proc, trn_score, val_score = \\\n",
    "            high_level_rf_reg_modeler(x_train_reg_proc,\n",
    "                                      y_train_reg,\n",
    "                                      x_test_reg_proc,\n",
    "                                      y_test_reg)\n",
    "        dump(model_reg, model_pickle_path)\n",
    "    else:\n",
    "        model_reg = load(model_pickle_path)\n",
    "\n",
    "    y_pred = model_reg.predict(x_test_reg_proc)\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def multi_model_keep_cols_regression(df_train_reg: pd.DataFrame,\n",
    "                                     df_test_reg: pd.DataFrame,\n",
    "                                     y_col: str,\n",
    "                                     rerun_model: bool,\n",
    "                                     model_pickle_path_1: str,\n",
    "                                     model_pickle_path_2: str) -> np.ndarray:\n",
    "    if regression_mode != 'model_2':\n",
    "        x_train_reg_1 =df_train_reg[reg_keep_cols_1]\n",
    "        # print(x_train_reg_1.head())\n",
    "        # print(x_train_reg_1.columns.tolist())\n",
    "        y_train_reg_1 = df_train_reg[y_col]\n",
    "\n",
    "        x_test_reg_1 =df_test_reg[reg_keep_cols_1]\n",
    "        # print(x_test_reg_1.head())\n",
    "        # print(x_test_reg_1.columns.tolist())\n",
    "        y_test_reg_1 = df_test_reg[y_col]\n",
    "\n",
    "        x_train_reg_proc_1, x_test_reg_proc_1 = \\\n",
    "            prepare_features_for_sklearn_modelling(x_train_reg_1, x_test_reg_1)\n",
    "        # print(x_test_reg_proc_1.head())\n",
    "        # print(x_test_reg_proc_1.head())\n",
    "\n",
    "        if rerun_model:\n",
    "            model_reg_1, trn_score_1, val_score_1 = \\\n",
    "                high_level_rf_reg_modeler(x_train_reg_proc_1,\n",
    "                                          y_train_reg_1,\n",
    "                                          x_test_reg_proc_1,\n",
    "                                          y_test_reg_1)\n",
    "            dump(model_reg_1, model_pickle_path_1)\n",
    "        else:\n",
    "            model_reg_1 = load(model_pickle_path_1)\n",
    "\n",
    "        y_pred_1 = model_reg_1.predict(x_test_reg_proc_1)\n",
    "        forecast_1 =df_test_reg.reset_index()[['invoice_date', 'pos_code']]\n",
    "        forecast_1['predicted_loading_1'] = y_pred_1\n",
    "\n",
    "    if regression_mode != 'model_1':\n",
    "        x_train_reg_2 =df_train_reg[reg_keep_cols_2]\n",
    "        # print(x_train_reg_2.head())\n",
    "        # print(x_train_reg_2.columns.tolist())\n",
    "        y_train_reg_2 = df_train_reg[y_col]\n",
    "\n",
    "        x_test_reg_2 =df_test_reg[reg_keep_cols_2]\n",
    "        # print(x_test_reg_2.head())\n",
    "        # print(x_test_reg_2.columns.tolist())\n",
    "        y_test_reg_2 = df_test_reg[y_col]\n",
    "\n",
    "        x_train_reg_proc_2, x_test_reg_proc_2 = \\\n",
    "            prepare_features_for_sklearn_modelling(x_train_reg_2, x_test_reg_2)\n",
    "        # print(x_train_reg_proc_2.head())\n",
    "        # print(x_test_reg_proc_2.head())\n",
    "\n",
    "        if rerun_model:\n",
    "            model_reg_2, trn_score_2, val_score_2 = \\\n",
    "                high_level_rf_reg_modeler(x_train_reg_proc_2,\n",
    "                                          y_train_reg_2,\n",
    "                                          x_test_reg_proc_2,\n",
    "                                          y_test_reg_2)\n",
    "            dump(model_reg_2, model_pickle_path_2)\n",
    "        else:\n",
    "            model_reg_2 = load(model_pickle_path_2)\n",
    "\n",
    "        y_pred_2 = model_reg_2.predict(x_test_reg_proc_2)\n",
    "        forecast_2 =df_test_reg.reset_index()[['invoice_date', 'pos_code']]\n",
    "        forecast_2['predicted_loading_2'] = y_pred_2\n",
    "\n",
    "    if regression_mode == 'model_1':\n",
    "        y_pred = y_pred_1\n",
    "    elif regression_mode == 'model_2':\n",
    "        y_pred = y_pred_2\n",
    "    elif regression_mode == 'ensemble':\n",
    "        temp = df_test_reg.reset_index()\n",
    "        assert (temp['pre_nonreplacement_holiday'].dtype == 'bool'\n",
    "                and temp['triple_sell'].dtype == 'bool'\n",
    "                and temp['double_sell'].dtype == 'bool')\n",
    "        special_days_mask = (temp['pre_nonreplacement_holiday']\n",
    "                             | temp['triple_sell']\n",
    "                             | temp['double_sell'])\n",
    "        special_days_mask = (special_days_mask\n",
    "                             | (temp['days_since_price_chg_ann'] >= 0)\n",
    "                             | (temp['credit_request_coeff'] > 0)\n",
    "                             | (temp['days_from_easter'] >= 0))\n",
    "        # assert (temp['is_bad_weather_last_three'].dtype == 'bool'\n",
    "        #         and temp['is_bad_weather_last_six'].dtype == 'bool')\n",
    "        # bad_weather_mask = (temp['is_bad_weather_last_three']\n",
    "        #                     | temp['is_bad_weather_last_six'])\n",
    "        # bad_weather_mask = (bad_weather_mask & ~special_days_mask)\n",
    "        forecast =forecast_1.rename(\n",
    "            columns={'predicted_loading_1': 'predicted_loading'})\n",
    "        forecast.loc[special_days_mask, 'predicted_loading'] = \\\n",
    "            [max(item) for item in zip(forecast_1.loc[special_days_mask, 'predicted_loading_1'],\n",
    "                                       forecast_2.loc[special_days_mask, 'predicted_loading_2'])]\n",
    "        # forecast.loc[bad_weather_mask, 'predicted_loading'] = forecast_2.loc[\n",
    "        #     bad_weather_mask, 'predicted_loading_2']\n",
    "        assert len(forecast) == len(y_pred_1) == len(y_pred_2)\n",
    "        y_pred = forecast['predicted_loading'].to_numpy()\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def model(df_train: pd.DataFrame,\n",
    "          df_test: pd.DataFrame,\n",
    "          y_col: str,\n",
    "          is_pred_date_first_in_period: bool,\n",
    "          extra_test_features_introduced: list,\n",
    "          model_period: str,\n",
    "          current_product: str,\n",
    "          so_code: str,\n",
    "          model_pickle_path: str,\n",
    "          run_log_path: str) -> tuple:\n",
    "\n",
    "    clf_model_pickle_path = os.path.join(\n",
    "        model_pickle_path,\n",
    "        'model_clf_{}_{}_{}.jpkl'.format(model_period, so_code, current_product))\n",
    "    if is_pred_date_first_in_period:\n",
    "        # Do training till last period marker & pickle the model for later use\n",
    "        df_test = nonzero_shipment_classification(\n",
    "            df_train,\n",
    "            df_test,\n",
    "            y_col,\n",
    "            clf_drop_cols,\n",
    "            extra_test_features_introduced,\n",
    "            current_product,\n",
    "            True,\n",
    "            clf_model_pickle_path,\n",
    "            run_log_path\n",
    "        )\n",
    "    else:\n",
    "        try:\n",
    "            df_test = nonzero_shipment_classification(\n",
    "                df_train,\n",
    "                df_test,\n",
    "                y_col,\n",
    "                clf_drop_cols,\n",
    "                extra_test_features_introduced,\n",
    "                current_product,\n",
    "                False,\n",
    "                clf_model_pickle_path,\n",
    "                run_log_path\n",
    "            )\n",
    "        except FileNotFoundError:\n",
    "            # Do training till last period marker & pickle the model for later use\n",
    "            df_test = nonzero_shipment_classification(\n",
    "                df_train,\n",
    "                df_test,\n",
    "                y_col,\n",
    "                clf_drop_cols,\n",
    "                extra_test_features_introduced,\n",
    "                current_product,\n",
    "                True,\n",
    "                clf_model_pickle_path,\n",
    "                run_log_path\n",
    "            )\n",
    "\n",
    "    # non-zero marked shipments forecast using regression\n",
    "    df_train_reg = df_train.loc[~df_train['is_zero_sale']].copy()\n",
    "    # Take only visit plan wise\n",
    "    df_test_reg = df_test.loc[(df_test['isVisitPlan']\n",
    "                               & df_test['pred_is_nonzero_shipments'])].copy()\n",
    "    if len(df_test_reg) == 0:\n",
    "        forecast_result =df_test.loc[\n",
    "            df_test['isVisitPlan']].rename(columns={'shipments': 'known_shipment'})\n",
    "        forecast_result = forecast_result.reset_index()\n",
    "        forecast_result['predicted_loading'] = 0\n",
    "        return df_test, forecast_result\n",
    "    if use_drop_reg_cols_instead_of_keep:\n",
    "        reg_model_pickle_path = os.path.join(\n",
    "            model_pickle_path,\n",
    "            'model_reg_{}_{}_{}.jpkl'.format(model_period, so_code, current_product))\n",
    "        if is_pred_date_first_in_period:\n",
    "            # Do training till last period marker & pickle the model for later use\n",
    "            y_pred = drop_cols_regression(\n",
    "                df_train_reg,\n",
    "                df_test_reg,\n",
    "                y_col,\n",
    "                extra_test_features_introduced,\n",
    "                True,\n",
    "                reg_model_pickle_path\n",
    "            )\n",
    "        else:\n",
    "            try:\n",
    "                y_pred = drop_cols_regression(\n",
    "                    df_train_reg,\n",
    "                    df_test_reg,\n",
    "                    y_col,\n",
    "                    extra_test_features_introduced,\n",
    "                    False,\n",
    "                    reg_model_pickle_path\n",
    "                )\n",
    "            except FileNotFoundError:\n",
    "                # Do training till last period marker & pickle the model for later use\n",
    "                y_pred = drop_cols_regression(\n",
    "                    df_train_reg,\n",
    "                    df_test_reg,\n",
    "                    y_col,\n",
    "                    extra_test_features_introduced,\n",
    "                    True,\n",
    "                    reg_model_pickle_path\n",
    "                )\n",
    "        forecast =df_test_reg.reset_index()[['invoice_date', 'pos_code']]\n",
    "        forecast['predicted_loading'] = y_pred\n",
    "    else:\n",
    "        reg_model_pickle_path_1 = os.path.join(\n",
    "            model_pickle_path,\n",
    "            'model_reg_{}_{}_{}_1.jpkl'.format(model_period, so_code, current_product))\n",
    "        reg_model_pickle_path_2 = os.path.join(\n",
    "            model_pickle_path,\n",
    "            'model_reg_{}_{}_{}_2.jpkl'.format(model_period, so_code, current_product))\n",
    "        if is_pred_date_first_in_period:\n",
    "            # Do training till last period marker & pickle the model for later use\n",
    "            y_pred = multi_model_keep_cols_regression(\n",
    "                df_train_reg,\n",
    "                df_test_reg,\n",
    "                y_col,\n",
    "                True,\n",
    "                reg_model_pickle_path_1,\n",
    "                reg_model_pickle_path_2\n",
    "            )\n",
    "        else:\n",
    "            try:\n",
    "                y_pred = multi_model_keep_cols_regression(\n",
    "                    df_train_reg,\n",
    "                    df_test_reg,\n",
    "                    y_col,\n",
    "                    False,\n",
    "                    reg_model_pickle_path_1,\n",
    "                    reg_model_pickle_path_2\n",
    "                )\n",
    "            except FileNotFoundError:\n",
    "                # Do training till last period marker & pickle the model for later use\n",
    "                y_pred = multi_model_keep_cols_regression(\n",
    "                    df_train_reg,\n",
    "                    df_test_reg,\n",
    "                    y_col,\n",
    "                    True,\n",
    "                    reg_model_pickle_path_1,\n",
    "                    reg_model_pickle_path_2\n",
    "                )\n",
    "    forecast =df_test_reg.reset_index()[['invoice_date', 'pos_code']]\n",
    "    forecast['predicted_loading'] = y_pred\n",
    "    forecast_result =df_test.loc[\n",
    "        df_test['isVisitPlan']].rename(columns={'shipments': 'known_shipment'})\n",
    "    forecast_result = forecast_result.merge(forecast,\n",
    "                                            how='left',\n",
    "                                            on=['invoice_date', 'pos_code'])\n",
    "    forecast_result['predicted_loading'] = forecast_result['predicted_loading'].fillna(0)\n",
    "\n",
    "    return df_test, forecast_result\n",
    "\n",
    "\n",
    "def high_level_post_model_operations(agent_forecasts: pd.DataFrame,\n",
    "                                     train_data_with_agent: pd.DataFrame,\n",
    "                                     past_test_data_with_agent: pd.DataFrame,\n",
    "                                     inflated_demand_marker_data: pd.DataFrame,\n",
    "                                     model_type: str,\n",
    "                                     product_code: str,\n",
    "                                     hyperparams_dict: dict,\n",
    "                                     int_result_dump_path: str,\n",
    "                                     run_log_path: str) :\n",
    "    assert agent_forecasts['agent_code'].nunique() == 1\n",
    "    assert agent_forecasts['visit_date'].nunique() == 1\n",
    "    current_pred_date = agent_forecasts['visit_date'].iloc[0]\n",
    "    current_agent = agent_forecasts['agent_code'].iloc[0]\n",
    "    current_product_cat = agent_forecasts['product_cat'].iloc[0]\n",
    "    # print('current agent:', current_agent)\n",
    "\n",
    "    agent_params: dict = hyperparams_dict[current_agent]\n",
    "    chosen_agg_buffer_pct = agent_params['agg_buffer_pct']\n",
    "    chosen_forced_loading_upper_limit = agent_params['force_upper_limit']\n",
    "    chosen_force_inflation_factor = agent_params['force_inflation_factor']\n",
    "    chosen_std_adjust_factor = agent_params['std_adjust_factor']\n",
    "\n",
    "    agg_forecast_df = __high_level_std_inflate_agg_wrapper(\n",
    "        agent_forecasts,\n",
    "        train_data_with_agent,\n",
    "        past_test_data_with_agent,\n",
    "        inflated_demand_marker_data,\n",
    "        current_pred_date,\n",
    "        model_type,\n",
    "        product_code,\n",
    "        current_agent,\n",
    "        int_result_dump_path,\n",
    "        chosen_agg_buffer_pct,\n",
    "        chosen_forced_loading_upper_limit,\n",
    "        chosen_force_inflation_factor,\n",
    "        chosen_std_adjust_factor\n",
    "    )\n",
    "\n",
    "    with open(run_log_path, 'a') as model_log:\n",
    "        model_log.write('hyperparams loaded from storage:\\n')\n",
    "        model_log.write('final params used for {}, {}: \\n'.format(current_agent, current_pred_date))\n",
    "        model_log.write('agg buffer pct: {}\\n'.format(chosen_agg_buffer_pct))\n",
    "        model_log.write('force upper limit: {}\\n'.format(chosen_forced_loading_upper_limit))\n",
    "        model_log.write('force inflation factor: {}\\n'.format(chosen_force_inflation_factor))\n",
    "        model_log.write('std adjust factor: {}\\n'.format(chosen_std_adjust_factor))\n",
    "\n",
    "    return agg_forecast_df\n",
    "\n",
    "\n",
    "def _flag_problematic_pos(train_data: pd.DataFrame,\n",
    "                          test_data: pd.DataFrame) -> tuple:\n",
    "    train_df = train_data.copy()\n",
    "    test_df = test_data.copy()\n",
    "\n",
    "    train_measures =train_df.groupby(\n",
    "        'pos_code').agg({'shipments': ['std', 'median', 'mean', 'count']})\n",
    "    train_measures.columns = ['train_' + '_'.join(col) for\n",
    "                              col in train_measures.columns.values]\n",
    "    train_measures['train_shipments_vp_count'] = \\\n",
    "        train_df[train_df['isVisitPlan']].groupby(\n",
    "            'pos_code').agg({'shipments': 'count'})\n",
    "    # print(train_measures)\n",
    "    test_df = test_df.merge(train_measures, how='left', on='pos_code')\n",
    "\n",
    "    train_dates_count = len(set(train_df.index))\n",
    "    train_df['shipments'] = train_df['shipments'].replace(0, np.nan)\n",
    "    train_df = train_df.dropna(subset=['shipments'])\n",
    "\n",
    "    train_nonzero_measures =train_df.groupby(\n",
    "        'pos_code').agg({'shipments': ['std', 'median', 'mean', 'count'],\n",
    "                         'isVisitPlan': 'sum'})\n",
    "    train_nonzero_measures.columns = ['train_nonzero_' + '_'.join(col) for\n",
    "                                      col in train_nonzero_measures.columns.values]\n",
    "    train_nonzero_measures['train_nonzero_shipments_vp_count'] = \\\n",
    "        train_df[train_df['isVisitPlan']].groupby(\n",
    "            'pos_code').agg({'shipments': 'count'})\n",
    "    # TODO: train_nonzero_shipments_vp_count is same as train_nonzero_isVisitPlan_sum\n",
    "    test_df = test_df.merge(train_nonzero_measures, how='left', on='pos_code')\n",
    "    test_df['Shipments_train_density'] = test_df['train_nonzero_shipments_count'] / train_dates_count\n",
    "    test_df['Shipments_train_sparsity'] = 1 - (test_df['train_nonzero_shipments_vp_count']\n",
    "                                               / test_df['train_shipments_vp_count'])\n",
    "    test_df['isVisitPlan_train_density'] = (test_df['train_nonzero_isVisitPlan_sum']\n",
    "                                            / test_df['train_nonzero_shipments_count'])\n",
    "    # print(test_df)\n",
    "\n",
    "    mask = ((test_df['train_nonzero_shipments_std']\n",
    "             >= test_df['train_nonzero_shipments_std'].quantile(high_std_thr))\n",
    "            | (test_df['train_shipments_std']\n",
    "               >= test_df['train_shipments_std'].quantile(high_std_thr))) \\\n",
    "           & ((test_df['Shipments_train_density'] <= low_density_thr)\n",
    "              | (test_df['isVisitPlan_train_density'] <= low_vp_thr))\n",
    "    problematic_pos = pd.Series(test_df.loc[mask, 'pos_code'].unique())\n",
    "\n",
    "    extra_test_features_introduced = ['train_nonzero_shipments_std',\n",
    "                                      'train_nonzero_shipments_median',\n",
    "                                      'train_nonzero_shipments_mean',\n",
    "                                      'train_nonzero_shipments_count',\n",
    "                                      'train_nonzero_isVisitPlan_sum',\n",
    "                                      'train_nonzero_shipments_vp_count',\n",
    "                                      'train_shipments_std',\n",
    "                                      'train_shipments_mean',\n",
    "                                      'train_shipments_median',\n",
    "                                      'train_shipments_count',\n",
    "                                      'train_shipments_vp_count',\n",
    "                                      'Shipments_train_density',\n",
    "                                      'Shipments_train_sparsity',\n",
    "                                      'isVisitPlan_train_density']\n",
    "\n",
    "    return problematic_pos, test_df, extra_test_features_introduced\n",
    "\n",
    "\n",
    "# Adjustments based on std\n",
    "def _post_model_std_based_adjust(forecast_result: pd.DataFrame,\n",
    "                                 std_adjustment_factor: float) -> pd.DataFrame:\n",
    "    forecast_result.loc[:, 'predicted_loading_std_adj_amt'] = 0\n",
    "    high_std_thr_val = forecast_result['train_nonzero_shipments_std'].quantile(high_std_thr)\n",
    "    high_overall_std_thr_val = forecast_result['train_shipments_std'].quantile(high_std_thr)\n",
    "\n",
    "    if std_adjustment_factor > 0:\n",
    "        std_adjust_mask = (((forecast_result['train_nonzero_shipments_std'] >= high_std_thr_val)\n",
    "                            | (forecast_result['train_shipments_std'] >= high_overall_std_thr_val))\n",
    "                           & (forecast_result['Shipments_train_density'] <= low_density_thr)\n",
    "                           & (forecast_result['isVisitPlan_train_density'] > low_vp_thr))\n",
    "        predicted_loading_std_adj = forecast_result.loc[\n",
    "                                        std_adjust_mask,\n",
    "                                        'train_nonzero_shipments_std'] * std_adjustment_factor\n",
    "        predicted_loading_std_adj = predicted_loading_std_adj.fillna(0)\n",
    "        forecast_result.loc[\n",
    "            std_adjust_mask, 'predicted_loading'] = forecast_result.loc[\n",
    "            std_adjust_mask, 'predicted_loading'] + predicted_loading_std_adj\n",
    "        forecast_result.loc[\n",
    "            std_adjust_mask, 'predicted_loading_std_adj_amt'] = predicted_loading_std_adj\n",
    "\n",
    "    return forecast_result\n",
    "\n",
    "\n",
    "# double/triple sell inflation before aggregation\n",
    "def _post_model_result_inflation(forecast_result: pd.DataFrame,\n",
    "                                 inflation_factor: float,\n",
    "                                 int_result_dump_path: str,\n",
    "                                 pred_date: pd.Timestamp,\n",
    "                                 model_type: str,\n",
    "                                 product_code: str,\n",
    "                                 agent_code = None) -> pd.DataFrame:\n",
    "    def __inflation_levels_list(row, result_column_name):\n",
    "        _inflation_factor = inflation_factor\n",
    "        if row['triple_sell'] or row['credit_request_type'] == 'TRIPLE':\n",
    "            _inflation_factor = _inflation_factor + 0.2\n",
    "\n",
    "        if not row['pred_is_nonzero_shipments']:\n",
    "            return [row[result_column_name] * _inflation_factor, 0]\n",
    "        else:\n",
    "            return [row[result_column_name] * _inflation_factor,\n",
    "                    (row[result_column_name]\n",
    "                     + row['train_nonzero_shipments_mean'] * (_inflation_factor - 1))]\n",
    "\n",
    "    forecast_result.loc[:, 'predicted_loading'] = \\\n",
    "        forecast_result.apply(lambda row:\n",
    "                              np.nanmax(__inflation_levels_list(row, 'predicted_loading'))\n",
    "                              if (row['triple_sell']\n",
    "                                  or row['double_sell']\n",
    "                                  or (row['credit_request_type'] == 'TRIPLE')\n",
    "                                  or (row['credit_request_type'] == 'DOUBLE')\n",
    "                                  or (row['days_from_easter'] >= 0)\n",
    "                                  or row['pre_nonreplacement_holiday'])\n",
    "\n",
    "                              else np.nanmean(__inflation_levels_list(row, 'predicted_loading'))\n",
    "                              if (row['days_since_price_chg_ann'] > 0\n",
    "                                  and row['days_from_price_chg'] > 0)\n",
    "\n",
    "                              else row['predicted_loading'],\n",
    "                              axis=1)\n",
    "\n",
    "    if agent_code:\n",
    "        forecast_result.to_csv(\n",
    "            os.path.join(int_result_dump_path,\n",
    "                         'result_with_zeroes_std_adjust_n_inflated_{}_{}_{}_{}_{}.csv'.format(\n",
    "                             inflation_factor, model_type, agent_code, product_code, pred_date.date()))\n",
    "        )\n",
    "    else:\n",
    "        forecast_result.to_csv(\n",
    "            os.path.join(int_result_dump_path,\n",
    "                         'result_with_zeroes_std_adjust_n_inflated_{}_{}_{}_{}.csv'.format(\n",
    "                             inflation_factor, model_type, product_code, pred_date.date()))\n",
    "        )\n",
    "    return forecast_result\n",
    "\n",
    "\n",
    "def __high_level_std_inflate_agg_wrapper(agent_forecasts: pd.DataFrame,\n",
    "                                         train_data_with_agent: pd.DataFrame,\n",
    "                                         past_test_data_with_agent: pd.DataFrame,\n",
    "                                         inflated_demand_marker_data: pd.DataFrame,\n",
    "                                         pred_date: pd.Timestamp,\n",
    "                                         model_type: str,\n",
    "                                         product_code: str,\n",
    "                                         agent_code: str,\n",
    "                                         int_result_dump_path: str,\n",
    "                                         agg_buffer_pct: float,\n",
    "                                         force_upper_limit: bool,\n",
    "                                         inflation_factor: float,\n",
    "                                         std_adjustment_factor: float) -> pd.DataFrame:\n",
    "    agent_forecasts = _post_model_std_based_adjust(\n",
    "        agent_forecasts,\n",
    "        std_adjustment_factor\n",
    "    )\n",
    "    agent_forecasts = _post_model_result_inflation(\n",
    "        agent_forecasts,\n",
    "        inflation_factor,\n",
    "        int_result_dump_path,\n",
    "        pred_date,\n",
    "        model_type,\n",
    "        product_code,\n",
    "        agent_code\n",
    "    )\n",
    "    agg_forecast_df =__high_level_agg(\n",
    "        agent_forecasts,\n",
    "        train_data_with_agent,\n",
    "        past_test_data_with_agent,\n",
    "        inflated_demand_marker_data,\n",
    "        product_code,\n",
    "        agg_buffer_pct,\n",
    "        force_upper_limit\n",
    "    )\n",
    "\n",
    "    return agg_forecast_df\n",
    "\n",
    "\n",
    "def __high_level_agg(agent_forecasts: pd.DataFrame,\n",
    "                     train_data_with_agent: pd.DataFrame,\n",
    "                     past_test_data_with_agent: pd.DataFrame,\n",
    "                     inflated_demand_marker_data: pd.DataFrame,\n",
    "                     product_code: str,\n",
    "                     agg_buffer_pct: float,\n",
    "                     force_upper_limit: bool) -> pd.DataFrame:\n",
    "\n",
    "    agg_forecast_df =high_level_post_model_aggregator(\n",
    "        agent_forecasts,\n",
    "        train_data_with_agent,\n",
    "        past_test_data_with_agent,\n",
    "        inflated_demand_marker_data,\n",
    "        agg_buffer_pct,\n",
    "        force_upper_limit\n",
    "    )\n",
    "    agg_forecast_df['product_code'] = product_code\n",
    "\n",
    "    return agg_forecast_df\n",
    "\n",
    "\n",
    "def __form_tuning_result_df(keys: list, values: list) -> pd.DataFrame:\n",
    "    assert len(keys) == len(values)\n",
    "    assert isinstance(values[0], list)\n",
    "    df_len = len(values[0])\n",
    "    if df_len > 1:\n",
    "        return pd.DataFrame(dict(zip(keys, values)))\n",
    "    elif df_len == 1:\n",
    "        return pd.DataFrame(dict(zip(keys, values)), index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "shipment_data = broadcasted_df.value[0][1]\n",
    "present_agent_map_data = broadcasted_df.value[1][1].copy()\n",
    "prepared_product_df = broadcasted_df.value[2][1].copy()\n",
    "inflated_demand_marker_data = broadcasted_df.value[3][1].copy()\n",
    "shipment_split_data = broadcasted_df.value[4][1].copy()\n",
    "sr_loading_data = broadcasted_df.value[5][1].copy()\n",
    "sr_unloading_data = broadcasted_df.value[6][1].copy()\n",
    "present_agent_map_data = present_agent_map_data.reset_index()\n",
    "\n",
    "prepared_product_data = prepared_product_df[prepared_product_df.product_code.isin([pro_code])].copy()\n",
    "prepared_product_data = prepared_product_data.set_index(['product_code', 'invoice_date']).sort_index()\n",
    "\n",
    "current_product = pro_code\n",
    "\n",
    "model_pickle_path = 'model_pickles'\n",
    "os.makedirs(model_pickle_path, exist_ok=True)\n",
    "# Model pickles are only utilised for reusing during the full run for 1 product\n",
    "# So at the start of run for a new product any existing model pickle file can be\n",
    "# discarded to save disk space.\n",
    "exisiting_model_pickle_files = os.listdir(model_pickle_path)\n",
    "if len(exisiting_model_pickle_files) > 0:\n",
    "    for model_pickle in exisiting_model_pickle_files:\n",
    "        file_path = os.path.join(model_pickle_path, model_pickle)\n",
    "        try:\n",
    "            if os.path.isfile(file_path):\n",
    "                os.unlink(file_path)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "int_result_dump_path = os.path.join(output_path, 'intermediate_dumps')\n",
    "os.makedirs(int_result_dump_path, exist_ok=True)  # succeeds even if directory exists.\n",
    "hyperparams_storage_path = 'hyperparams'\n",
    "os.makedirs(hyperparams_storage_path, exist_ok=True)\n",
    "\n",
    "current_product = prepared_product_data.index.get_level_values(0)[0]\n",
    "# print(\"current_product\", current_product)\n",
    "product_df =prepared_product_data.copy()\n",
    "product_df.index = product_df.index.droplevel()\n",
    "product_df = product_df.sort_index()\n",
    "dates_in_data: pd.DatetimeIndex = product_df.index\n",
    "pred_dates = dates_in_data[dates_in_data >= pred_start_date].unique().array\n",
    "daywise_forecast_df_list: list = []\n",
    "daywise_forecast_raw_annual_model_list: list = []\n",
    "daywise_forecast_raw_monthly_model_list: list = []\n",
    "daywise_forecast_raw_weekly_model_list: list = []\n",
    "\n",
    "past_agent_map =shipment_data.loc[shipment_data['product_code'] == current_product]\n",
    "past_agent_map = past_agent_map.reset_index()\n",
    "past_agent_map = past_agent_map[['invoice_date', 'pos_code', 'agent_code']]\n",
    "\n",
    "with open(os.path.join(\n",
    "        hyperparams_storage_path,\n",
    "        '{}_{}_hyperparams.json'.format(so_code, current_product)), 'r') as fp:\n",
    "    hyperparams_dict: dict = json.load(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2018-01-04 00:00:00')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_date = pred_dates[1]\n",
    "pred_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Souvik\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\Souvik\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\Souvik\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\Souvik\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\Souvik\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\Souvik\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "daywise_forecasts = high_level_daywise_model_handler(\n",
    "            product_df,\n",
    "            inflated_demand_marker_data,\n",
    "            present_agent_map_data,\n",
    "            past_agent_map,\n",
    "            pred_date,\n",
    "            remove_first_month_from_training,\n",
    "            y_col,\n",
    "            hyperparams_dict,\n",
    "            current_product,\n",
    "            so_code,\n",
    "            run_log_path,\n",
    "            model_pickle_path,\n",
    "            int_result_dump_path\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High Level Handlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def high_level_model_handler(pro_code):\n",
    "    \n",
    "    shipment_data = broadcasted_df.value[0][1]\n",
    "    present_agent_map_data = broadcasted_df.value[1][1].copy()\n",
    "    prepared_product_df = broadcasted_df.value[2][1].copy()\n",
    "    inflated_demand_marker_data = broadcasted_df.value[3][1].copy()\n",
    "    shipment_split_data = broadcasted_df.value[4][1].copy()\n",
    "    sr_loading_data = broadcasted_df.value[5][1].copy()\n",
    "    sr_unloading_data = broadcasted_df.value[6][1].copy()\n",
    "    present_agent_map_data = present_agent_map_data.reset_index()\n",
    "\n",
    "    prepared_product_data = prepared_product_df[prepared_product_df.product_code.isin([pro_code])].copy()\n",
    "    prepared_product_data = prepared_product_data.set_index(['product_code', 'invoice_date']).sort_index()\n",
    "\n",
    "    current_product = pro_code\n",
    "    \n",
    "    model_pickle_path = 'model_pickles'\n",
    "    os.makedirs(model_pickle_path, exist_ok=True)\n",
    "    # Model pickles are only utilised for reusing during the full run for 1 product\n",
    "    # So at the start of run for a new product any existing model pickle file can be\n",
    "    # discarded to save disk space.\n",
    "    exisiting_model_pickle_files = os.listdir(model_pickle_path)\n",
    "    if len(exisiting_model_pickle_files) > 0:\n",
    "        for model_pickle in exisiting_model_pickle_files:\n",
    "            file_path = os.path.join(model_pickle_path, model_pickle)\n",
    "            try:\n",
    "                if os.path.isfile(file_path):\n",
    "                    os.unlink(file_path)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "    int_result_dump_path = os.path.join(output_path, 'intermediate_dumps')\n",
    "    os.makedirs(int_result_dump_path, exist_ok=True)  # succeeds even if directory exists.\n",
    "    hyperparams_storage_path = 'hyperparams'\n",
    "    os.makedirs(hyperparams_storage_path, exist_ok=True)\n",
    "\n",
    "    current_product = prepared_product_data.index.get_level_values(0)[0]\n",
    "    # print(\"current_product\", current_product)\n",
    "    product_df =prepared_product_data.copy()\n",
    "    product_df.index = product_df.index.droplevel()\n",
    "    product_df = product_df.sort_index()\n",
    "    dates_in_data: pd.DatetimeIndex = product_df.index\n",
    "    pred_dates = dates_in_data[dates_in_data >= pred_start_date].unique().array\n",
    "    daywise_forecast_df_list: list = []\n",
    "    daywise_forecast_raw_annual_model_list: list = []\n",
    "    daywise_forecast_raw_monthly_model_list: list = []\n",
    "    daywise_forecast_raw_weekly_model_list: list = []\n",
    "\n",
    "    past_agent_map =shipment_data.loc[shipment_data['product_code'] == current_product]\n",
    "    past_agent_map = past_agent_map.reset_index()\n",
    "    past_agent_map = past_agent_map[['invoice_date', 'pos_code', 'agent_code']]\n",
    "\n",
    "    with open(os.path.join(\n",
    "            hyperparams_storage_path,\n",
    "            '{}_{}_hyperparams.json'.format(so_code, current_product)), 'r') as fp:\n",
    "        hyperparams_dict: dict = json.load(fp)\n",
    "\n",
    "    # TODO: record date wise runtime\n",
    "    for pred_date in tqdm(pred_dates):\n",
    "        # print(pred_date)\n",
    "        daywise_forecasts = high_level_daywise_model_handler(\n",
    "            product_df,\n",
    "            inflated_demand_marker_data,\n",
    "            present_agent_map_data,\n",
    "            past_agent_map,\n",
    "            pred_date,\n",
    "            remove_first_month_from_training,\n",
    "            y_col,\n",
    "            hyperparams_dict,\n",
    "            current_product,\n",
    "            so_code,\n",
    "            run_log_path,\n",
    "            model_pickle_path,\n",
    "            int_result_dump_path\n",
    "        )\n",
    "        if daywise_forecasts is not None:\n",
    "            curr_forecast_df, curr_forecast_raw_annual_model, \\\n",
    "                curr_forecast_raw_monthly_model, curr_forecast_raw_weekly_model = daywise_forecasts\n",
    "            daywise_forecast_df_list.append(curr_forecast_df)\n",
    "            daywise_forecast_raw_annual_model_list.append(curr_forecast_raw_annual_model)\n",
    "            daywise_forecast_raw_monthly_model_list.append(curr_forecast_raw_monthly_model)\n",
    "            daywise_forecast_raw_weekly_model_list.append(curr_forecast_raw_weekly_model)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    if len(daywise_forecast_df_list) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        forecast_df =pd.concat(daywise_forecast_df_list, sort=False)\n",
    "        forecast_raw_annual_model =pd.concat(daywise_forecast_raw_annual_model_list, sort=False)\n",
    "        forecast_raw_annual_model.to_csv(\n",
    "            os.path.join(int_result_dump_path,\n",
    "                         'result_with_zeroes_annual_model_{}.csv'.format(current_product))\n",
    "        )\n",
    "        forecast_raw_monthly_model =pd.concat(daywise_forecast_raw_monthly_model_list, sort=False)\n",
    "        forecast_raw_monthly_model.to_csv(\n",
    "            os.path.join(int_result_dump_path,\n",
    "                         'result_with_zeroes_monthly_model_{}.csv'.format(current_product))\n",
    "        )\n",
    "        forecast_raw_weekly_model =pd.concat(daywise_forecast_raw_weekly_model_list, sort=False)\n",
    "        forecast_raw_weekly_model.to_csv(\n",
    "            os.path.join(int_result_dump_path,\n",
    "                         'result_with_zeroes_weekly_model_{}.csv'.format(current_product))\n",
    "        )\n",
    "\n",
    "        forecast_comparison_df = high_level_post_agg_comparison_prep(\n",
    "            forecast_df,\n",
    "            current_product,\n",
    "            shipment_data,\n",
    "            shipment_split_data,\n",
    "            sr_loading_data,\n",
    "            sr_unloading_data\n",
    "        )\n",
    "        asl_unload_pct, asl_oos_pct, jti_unload_pct, jti_oos_pct = get_forecast_performance_figures(\n",
    "            forecast_comparison_df\n",
    "        )\n",
    "        asl_cost = asl_unload_pct + asl_oos_pct\n",
    "        jti_cost = jti_unload_pct + jti_oos_pct\n",
    "        with open(run_log_path, 'a') as model_log:\n",
    "            model_log.write('product {} final result:'.format(current_product))\n",
    "            model_log.write('asl_unload_pct: ' + str(asl_unload_pct)\n",
    "                            + 'asl_oos_pct: ' + str(asl_oos_pct)\n",
    "                            + 'asl_cost: ' + str(asl_cost) + '\\n')\n",
    "            model_log.write('jti_unload_pct: ' + str(jti_unload_pct)\n",
    "                            + 'jti_oos_pct: ' + str(jti_oos_pct)\n",
    "                            + 'jti_cost: ' + str(jti_cost) + '\\n')\n",
    "        forecast_comparison_df.to_csv(os.path.join(\n",
    "            output_path,\n",
    "            'final_output_rf_high_no_pos_removed_{}.csv'.format(current_product)))\n",
    "\n",
    "        return forecast_comparison_df\n",
    "\n",
    "\n",
    "def high_level_daywise_model_handler(product_data: pd.DataFrame,\n",
    "                                     inflated_demand_marker_data: pd.DataFrame,\n",
    "                                     present_agent_map_data: pd.DataFrame,\n",
    "                                     past_agent_map_data: pd.DataFrame,\n",
    "                                     pred_date: pd.Timestamp,\n",
    "                                     remove_first_month_from_training: bool,\n",
    "                                     y_col: str,\n",
    "                                     hyperparams_dict: dict,\n",
    "                                     product_code: str,\n",
    "                                     so_code: str,\n",
    "                                     run_log_path: str,\n",
    "                                     model_pickle_path: str,\n",
    "                                     int_result_dump_path: str):\n",
    "    try:\n",
    "        # Annual model section\n",
    "        annual_model_forecasts = single_date_split_model_forecast(\n",
    "            product_data,\n",
    "            inflated_demand_marker_data,\n",
    "            present_agent_map_data,\n",
    "            past_agent_map_data,\n",
    "            pred_date,\n",
    "            remove_first_month_from_training,\n",
    "            y_col,\n",
    "            'annual',\n",
    "            hyperparams_dict,\n",
    "            product_code,\n",
    "            so_code,\n",
    "            model_pickle_path,\n",
    "            run_log_path,\n",
    "            int_result_dump_path\n",
    "        )\n",
    "        if annual_model_forecasts is not None:\n",
    "            forecast_raw_annual_model, forecast_df_annual_model = annual_model_forecasts\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "        # Monthly model section\n",
    "        monthly_model_forecasts = single_date_split_model_forecast(\n",
    "            product_data,\n",
    "            inflated_demand_marker_data,\n",
    "            present_agent_map_data,\n",
    "            past_agent_map_data,\n",
    "            pred_date,\n",
    "            remove_first_month_from_training,\n",
    "            y_col,\n",
    "            'monthly',\n",
    "            hyperparams_dict,\n",
    "            product_code,\n",
    "            so_code,\n",
    "            model_pickle_path,\n",
    "            run_log_path,\n",
    "            int_result_dump_path\n",
    "        )\n",
    "        if monthly_model_forecasts is not None:\n",
    "            forecast_raw_monthly_model, forecast_df_monthly_model = monthly_model_forecasts\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "        # Weekly model section\n",
    "        weekly_model_forecasts = single_date_split_model_forecast(\n",
    "            product_data,\n",
    "            inflated_demand_marker_data,\n",
    "            present_agent_map_data,\n",
    "            past_agent_map_data,\n",
    "            pred_date,\n",
    "            remove_first_month_from_training,\n",
    "            y_col,\n",
    "            'weekly',\n",
    "            hyperparams_dict,\n",
    "            product_code,\n",
    "            so_code,\n",
    "            model_pickle_path,\n",
    "            run_log_path,\n",
    "            int_result_dump_path\n",
    "        )\n",
    "        if weekly_model_forecasts is not None:\n",
    "            forecast_raw_weekly_model, forecast_df_weekly_model = weekly_model_forecasts\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "        assert len(forecast_df_annual_model) == len(forecast_df_monthly_model) == len(forecast_df_weekly_model)\n",
    "        # check if some other columns need to be dropped\n",
    "        forecast_df =forecast_df_annual_model.drop(columns=['predicted_loading'])\n",
    "        forecast_df.loc[:, 'predicted_loading_annual_model'] = forecast_df_annual_model['predicted_loading'].array\n",
    "        forecast_df.loc[:, 'predicted_loading_monthly_model'] = forecast_df_monthly_model['predicted_loading'].array\n",
    "        forecast_df.loc[:, 'predicted_loading_weekly_model'] = forecast_df_weekly_model['predicted_loading'].array\n",
    "        forecast_df['predicted_loading'] = forecast_df[\n",
    "            ['predicted_loading_annual_model',\n",
    "             'predicted_loading_monthly_model',\n",
    "             'predicted_loading_weekly_model']].mean(axis=1, skipna=True)\n",
    "        forecast_df['predicted_loading'] = forecast_df['predicted_loading'].round(1)\n",
    "\n",
    "        return forecast_df, forecast_raw_annual_model, forecast_raw_monthly_model, forecast_raw_weekly_model\n",
    "    except Exception as e:\n",
    "        print('error in {}: check log'.format(product_code))\n",
    "        with open(run_log_path, 'a') as model_log:\n",
    "            model_log.write(product_code + ' ' + str(pred_date) + ' error encountered: \\n')\n",
    "            model_log.write(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def high_level_daywise_model_handler(product_data: pd.DataFrame,\n",
    "                                     inflated_demand_marker_data: pd.DataFrame,\n",
    "                                     present_agent_map_data: pd.DataFrame,\n",
    "                                     past_agent_map_data: pd.DataFrame,\n",
    "                                     pred_date: pd.Timestamp,\n",
    "                                     remove_first_month_from_training: bool,\n",
    "                                     y_col: str,\n",
    "                                     hyperparams_dict: dict,\n",
    "                                     product_code: str,\n",
    "                                     so_code: str,\n",
    "                                     run_log_path: str,\n",
    "                                     model_pickle_path: str,\n",
    "                                     int_result_dump_path: str) :\n",
    "    try:\n",
    "        # Annual model section\n",
    "        annual_model_forecasts = single_date_split_model_forecast(\n",
    "            product_data,\n",
    "            inflated_demand_marker_data,\n",
    "            present_agent_map_data,\n",
    "            past_agent_map_data,\n",
    "            pred_date,\n",
    "            remove_first_month_from_training,\n",
    "            y_col,\n",
    "            'annual',\n",
    "            hyperparams_dict,\n",
    "            product_code,\n",
    "            so_code,\n",
    "            model_pickle_path,\n",
    "            run_log_path,\n",
    "            int_result_dump_path\n",
    "        )\n",
    "        if annual_model_forecasts is not None:\n",
    "            forecast_raw_annual_model, forecast_df_annual_model = annual_model_forecasts\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "        # Monthly model section\n",
    "        monthly_model_forecasts = single_date_split_model_forecast(\n",
    "            product_data,\n",
    "            inflated_demand_marker_data,\n",
    "            present_agent_map_data,\n",
    "            past_agent_map_data,\n",
    "            pred_date,\n",
    "            remove_first_month_from_training,\n",
    "            y_col,\n",
    "            'monthly',\n",
    "            hyperparams_dict,\n",
    "            product_code,\n",
    "            so_code,\n",
    "            model_pickle_path,\n",
    "            run_log_path,\n",
    "            int_result_dump_path\n",
    "        )\n",
    "        if monthly_model_forecasts is not None:\n",
    "            forecast_raw_monthly_model, forecast_df_monthly_model = monthly_model_forecasts\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "        # Weekly model section\n",
    "        weekly_model_forecasts = single_date_split_model_forecast(\n",
    "            product_data,\n",
    "            inflated_demand_marker_data,\n",
    "            present_agent_map_data,\n",
    "            past_agent_map_data,\n",
    "            pred_date,\n",
    "            remove_first_month_from_training,\n",
    "            y_col,\n",
    "            'weekly',\n",
    "            hyperparams_dict,\n",
    "            product_code,\n",
    "            so_code,\n",
    "            model_pickle_path,\n",
    "            run_log_path,\n",
    "            int_result_dump_path\n",
    "        )\n",
    "        if weekly_model_forecasts is not None:\n",
    "            forecast_raw_weekly_model, forecast_df_weekly_model = weekly_model_forecasts\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "        assert len(forecast_df_annual_model) == len(forecast_df_monthly_model) == len(forecast_df_weekly_model)\n",
    "        # check if some other columns need to be dropped\n",
    "        forecast_df =forecast_df_annual_model.drop(columns=['predicted_loading'])\n",
    "        forecast_df.loc[:, 'predicted_loading_annual_model'] = forecast_df_annual_model['predicted_loading'].array\n",
    "        forecast_df.loc[:, 'predicted_loading_monthly_model'] = forecast_df_monthly_model['predicted_loading'].array\n",
    "        forecast_df.loc[:, 'predicted_loading_weekly_model'] = forecast_df_weekly_model['predicted_loading'].array\n",
    "        forecast_df['predicted_loading'] = forecast_df[\n",
    "            ['predicted_loading_annual_model',\n",
    "             'predicted_loading_monthly_model',\n",
    "             'predicted_loading_weekly_model']].mean(axis=1, skipna=True)\n",
    "        forecast_df['predicted_loading'] = forecast_df['predicted_loading'].round(1)\n",
    "\n",
    "        return forecast_df, forecast_raw_annual_model, forecast_raw_monthly_model, forecast_raw_weekly_model\n",
    "    except Exception as e:\n",
    "        print('error in {}: check log'.format(product_code))\n",
    "        with open(run_log_path, 'a') as model_log:\n",
    "            model_log.write(product_code + ' ' + str(pred_date) + ' error encountered: \\n')\n",
    "            model_log.write(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert strings into categorical for training data\n",
    "def _categorize_train(x_train_df: pd.DataFrame,\n",
    "                      y_train_series = pd.Series(),\n",
    "                      x_skip_cols = None) -> dict:\n",
    "    x_train_df = x_train_df.copy()\n",
    "    y_train_series = y_train_series.copy()\n",
    "    if x_skip_cols is None:\n",
    "        x_skip_cols = []\n",
    "\n",
    "    for col, series in x_train_df.items():\n",
    "        if col not in x_skip_cols:\n",
    "            if is_string_dtype(series):\n",
    "                x_train_df[col] = series.astype('category').cat.as_ordered()\n",
    "\n",
    "    categorize_train_dict = {'x_train_df': x_train_df,\n",
    "                             'y_train_series': y_train_series,\n",
    "                             'x_skip_cols': x_skip_cols}\n",
    "\n",
    "    return categorize_train_dict\n",
    "\n",
    "\n",
    "# Convert strings into categorical for validation data\n",
    "def _categorize_val(categorize_train_dict: dict,\n",
    "                    x_val_df: pd.DataFrame,\n",
    "                    y_val_series = None) -> tuple:\n",
    "    x_val_df = x_val_df.copy()\n",
    "    if y_val_series is None:\n",
    "        y_val_series = pd.Series()\n",
    "    else:\n",
    "        y_val_series = y_val_series.copy()\n",
    "\n",
    "    x_train_df = categorize_train_dict['x_train_df']\n",
    "    y_train_series = categorize_train_dict['y_train_series']\n",
    "    x_skip_cols = categorize_train_dict['x_skip_cols']\n",
    "\n",
    "    for col, series in x_val_df.items():\n",
    "        if (col in x_train_df.columns) and (col not in x_skip_cols):\n",
    "            if x_train_df[col].dtype.name == 'category':\n",
    "                x_val_df[col] = pd.Categorical(series,\n",
    "                                               categories=x_train_df[col].cat.categories,\n",
    "                                               ordered=True)\n",
    "\n",
    "    return x_val_df, y_val_series\n",
    "\n",
    "\n",
    "# Common non-text preprocessing for training data\n",
    "def _data_preproc_train(x_train_df: pd.DataFrame,\n",
    "                        y_train_series = pd.Series(),\n",
    "                        x_skip_cols = None,\n",
    "                        max_cat_count: int = 0) -> dict:\n",
    "    x_train_df = x_train_df.copy()\n",
    "    y_train_series = y_train_series.copy()\n",
    "    if x_skip_cols is None:\n",
    "        x_skip_cols = []\n",
    "\n",
    "    x_train_df.drop(x_skip_cols, axis=1, inplace=True)\n",
    "\n",
    "    for col, series in x_train_df.items():\n",
    "        if (not is_numeric_dtype(series)\n",
    "                and series.nunique() > max_cat_count):\n",
    "            x_train_df[col] = series.cat.codes + 1\n",
    "    x_train_df = pd.get_dummies(x_train_df, dummy_na=True)\n",
    "\n",
    "    if len(y_train_series) != 0:\n",
    "        if not is_numeric_dtype(y_train_series):\n",
    "            y_train_series = y_train_series.cat.codes\n",
    "\n",
    "    data_preproc_train_dict = {'x_train_df': x_train_df,\n",
    "                               'y_train_series': y_train_series,\n",
    "                               'x_skip_cols': x_skip_cols,\n",
    "                               'max_n_cat': max_cat_count}\n",
    "\n",
    "    return data_preproc_train_dict\n",
    "\n",
    "\n",
    "# Ensure validation data has the same columns as the training data\n",
    "def _handle_missing_cols(x_train_df: pd.DataFrame,\n",
    "                         x_val_df: pd.DataFrame,\n",
    "                         fill_value = np.nan) -> pd.DataFrame:\n",
    "    missing_cols = set(x_train_df.columns) - set(x_val_df.columns)\n",
    "    for col in missing_cols:\n",
    "        x_val_df[col] = fill_value\n",
    "    return x_val_df[x_train_df.columns]\n",
    "\n",
    "\n",
    "# Common non-text preprocessing for validation data\n",
    "def _data_preproc_val(data_preproc_train_dict: dict,\n",
    "                      x_val_df: pd.DataFrame,\n",
    "                      y_val_series = None) -> list:\n",
    "    x_val_df = x_val_df.copy()\n",
    "    if y_val_series is None:\n",
    "        y_val_series = pd.Series()\n",
    "    else:\n",
    "        y_val_series = y_val_series.copy()\n",
    "\n",
    "    x_train_df = data_preproc_train_dict['x_train_df']\n",
    "    y_train_series = data_preproc_train_dict['y_train_series']\n",
    "    x_skip_cols = data_preproc_train_dict['x_skip_cols']\n",
    "    max_cat_count: int = data_preproc_train_dict['max_n_cat']\n",
    "\n",
    "    x_val_df.drop(x_skip_cols, axis=1, inplace=True)\n",
    "\n",
    "    for col, series in x_val_df.items():\n",
    "        if (not is_numeric_dtype(series)\n",
    "                and series.nunique() > max_cat_count):\n",
    "            x_val_df[col] = series.cat.codes + 1\n",
    "    x_val_df = pd.get_dummies(x_val_df, dummy_na=True)\n",
    "    x_val_df = _handle_missing_cols(x_train_df, x_val_df)\n",
    "    x_val_df.fillna(-1, inplace=True)\n",
    "\n",
    "    if len(y_val_series) != 0:\n",
    "        if not is_numeric_dtype(y_val_series):\n",
    "            y_val_series = y_val_series.cat.codes\n",
    "            y_val_series.fillna(-1, inplace=True)\n",
    "\n",
    "    return [x_val_df, y_val_series]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def high_level_rf_clf_modeler(x_train: pd.DataFrame,\n",
    "                              y_train_df: pd.Series,\n",
    "                              x_test: pd.DataFrame,\n",
    "                              y_test_df: pd.Series) -> tuple:\n",
    "    y_train = y_train_df.astype(bool)\n",
    "    y_train = y_train.values.reshape(1, -1)[0]\n",
    "    y_test = y_test_df.astype(bool)\n",
    "    y_test = y_test.values.reshape(1, -1)[0]\n",
    "\n",
    "    clf_train_true_pct = sum(y_train) / len(y_train)\n",
    "    clf_test_true_pct = sum(y_test) / len(y_test)\n",
    "\n",
    "    model_clf, training_score, acc_clf, \\\n",
    "        clf_report, ck_score = rf_classifier(x_train, y_train, x_test, y_test)\n",
    "\n",
    "    return model_clf, clf_train_true_pct, clf_test_true_pct, \\\n",
    "        training_score, acc_clf, clf_report, ck_score\n",
    "\n",
    "\n",
    "def high_level_rf_reg_modeler(x_train: pd.DataFrame,\n",
    "                              y_train_df: pd.Series,\n",
    "                              x_test: pd.DataFrame,\n",
    "                              y_test_df: pd.Series) -> tuple:\n",
    "    y_train = y_train_df.values.reshape(1, -1)[0]\n",
    "    y_test = y_test_df.values.reshape(1, -1)[0]\n",
    "\n",
    "    model_rf, trn_score_rf, acc_rf = rf_regressor(x_train, y_train,\n",
    "                                                  x_test, y_test)\n",
    "\n",
    "    forecast_type = 'rf_regression'\n",
    "\n",
    "    # y_pred = model_rf.predict(x_test)\n",
    "    # result = pd.DataFrame(y_pred,\n",
    "    #                                     columns=['predicted_loading'],\n",
    "    #                                     index=y_test_df.index)\n",
    "    # result['known_shipment'] = y_test_df\n",
    "    # print(result)\n",
    "\n",
    "    return model_rf, trn_score_rf, acc_rf\n",
    "\n",
    "\n",
    "def high_level_event_product_rough_estimate_modeler(event_product_forecast_comparison_data: pd.DataFrame,\n",
    "                                                    train_data_with_agent: pd.DataFrame,\n",
    "                                                    train_start_date: pd.Timestamp,\n",
    "                                                    train_end_date: pd.Timestamp):\n",
    "    def agentwise_rough_estimate(product_agent_test_df: pd.DataFrame,\n",
    "                                 train_data_agent: pd.DataFrame) -> Optional[pd.DataFrame]:\n",
    "        current_agent = product_agent_test_df['agent_code'].iloc[0]\n",
    "        keep_in_test_df = ['visit_date', 'product_code', 'pre_nonreplacement_holiday',\n",
    "                           'week', 'month', 'year',\n",
    "                           'NationalDoubleSell', 'NationalTripleSell',\n",
    "                           'predicted_event_normal']\n",
    "        test_df = product_agent_test_df.loc[:, keep_in_test_df].copy()\n",
    "        train_df = train_data_agent.loc[\n",
    "            train_data_agent['agent_code'] == current_agent].copy()\n",
    "        if len(train_df) == 0:\n",
    "            # no training data\n",
    "            return None\n",
    "\n",
    "        model_train_df = train_df.groupby('visit_date').agg({'shipments': 'sum',\n",
    "                                                             'week': 'first',\n",
    "                                                             'month': 'first',\n",
    "                                                             'year': 'first'})\n",
    "        full_train_index = pd.date_range(train_start_date, train_end_date)\n",
    "        model_train_df = model_train_df.reindex(full_train_index)\n",
    "        model_train_df['shipments'] = model_train_df['shipments'].fillna(0)\n",
    "        model_train_df['week'] = model_train_df.index.weekofyear\n",
    "        model_train_df['month'] = model_train_df.index.month\n",
    "        model_train_df['year'] = model_train_df.index.year\n",
    "        model_train_df['shipments_log'] = np.log(model_train_df['shipments'].clip(0.001, None))\n",
    "\n",
    "        train_group = model_train_df.groupby(['week']).agg({'shipments_log': ['mean']})\n",
    "        train_group.columns = ['shipments_log_weekly_mean']\n",
    "        test_df = pd.merge(test_df, train_group.reset_index(),\n",
    "                           how='left', on='week')\n",
    "        train_group = model_train_df.groupby(['month']).agg({'shipments_log': ['max']})\n",
    "        train_group.columns = ['shipments_log_monthly_max']\n",
    "        test_df = pd.merge(test_df, train_group.reset_index(),\n",
    "                           how='left', on='month')\n",
    "        test_df['predicted_loading'] = np.exp(test_df['shipments_log_weekly_mean']).clip(\n",
    "            np.exp(test_df['shipments_log_monthly_max']), None)\n",
    "        test_df.loc[(test_df['NationalDoubleSell']\n",
    "                     | (test_df['pre_nonreplacement_holiday'] > 0)),\n",
    "                    'predicted_loading'] = test_df.loc[(test_df['NationalDoubleSell']\n",
    "                                                        | (test_df['pre_nonreplacement_holiday'] > 0)),\n",
    "                                                       'predicted_loading'] * 1.3\n",
    "        test_df.loc[test_df['NationalTripleSell'],\n",
    "                    'predicted_loading'] = test_df.loc[test_df['NationalTripleSell'],\n",
    "                                                       'predicted_loading'] * 1.5\n",
    "        return test_df.set_index(['visit_date'])\n",
    "\n",
    "    rough_estimates = event_product_forecast_comparison_data.groupby(\n",
    "        'agent_code').apply(agentwise_rough_estimate, train_data_with_agent)\n",
    "\n",
    "    return rough_estimates\n",
    "\n",
    "\n",
    "def post_model_sparse_modeler(date_agent_product_data: pd.DataFrame,\n",
    "                              prepared_ts_data: pd.DataFrame):\n",
    "    drop_cols = ['pos_product', 'characteristics', 'forecast_type']\n",
    "    return_cols = ['predicted_loading', 'double_sell',\n",
    "                   'triple_sell', 'price_chg_effect']\n",
    "    assert (date_agent_product_data['double_sell'].dtype == 'bool') and \\\n",
    "           (date_agent_product_data['triple_sell'].dtype == 'bool') and \\\n",
    "           (date_agent_product_data['weekday_holiday'].dtype == 'bool')\n",
    "\n",
    "    if date_agent_product_data['weekday_holiday'].iloc[0]:\n",
    "        return None\n",
    "    dt_agent_prod_df = date_agent_product_data.drop(columns=drop_cols)\n",
    "\n",
    "    # print(dt_agent_prod_df)\n",
    "    sparse_model_entries = dt_agent_prod_df[dt_agent_prod_df['predicted_loading'].isnull()]\n",
    "    norm_model_entries = dt_agent_prod_df[~dt_agent_prod_df['predicted_loading'].isnull()]\n",
    "    # print(sparse_model_entries)\n",
    "\n",
    "    if len(sparse_model_entries) > 0:\n",
    "        product_code = sparse_model_entries['product_code'].iloc[0]\n",
    "        visit_date = sparse_model_entries['visit_date'].iloc[0]\n",
    "        pos_product_list = [x + '_' + product_code for x in\n",
    "                            sparse_model_entries['pos_code'].values]\n",
    "\n",
    "        # take care to take train data only till the last date before current visit_date\n",
    "        # here that is done by taking a final .iloc[:-1] slice\n",
    "        sparse_model_train_list = [prepared_ts_data.loc[\n",
    "                                   ([pos_product],\n",
    "                                    slice(visit_date)),\n",
    "                                   ['quantity',\n",
    "                                    'pos_code',\n",
    "                                    'double_sell',\n",
    "                                    'triple_sell',\n",
    "                                    'weekday_holiday',\n",
    "                                    'price_chg_effect']].iloc[:-1] for pos_product in\n",
    "                                   pos_product_list]\n",
    "        # print(sparse_model_train_list)\n",
    "        # remove the pos_product info from all train timeseries\n",
    "        # so that bare timeseries are left\n",
    "        sparse_list = []\n",
    "        discarded_sparse_list = []\n",
    "        for sparse_train_ts in sparse_model_train_list:\n",
    "            sparse_train_ts.index = sparse_train_ts.index.droplevel()\n",
    "            # not_considered_mask = (sparse_train_ts['double_sell'] |\n",
    "            #                        sparse_train_ts['triple_sell'] |\n",
    "            #                        sparse_train_ts['weekday_holiday'])\n",
    "            # sparse_train_ts = sparse_train_ts.loc[~not_considered_mask]\n",
    "            if sparse_train_ts['quantity'].replace(0, np.nan).mean() >= 10:\n",
    "                discarded_sparse_list.append(sparse_train_ts['pos_code'].iloc[0])\n",
    "            else:\n",
    "                sparse_train_ts = sparse_train_ts[['quantity']]\n",
    "                sparse_list.append(sparse_train_ts)\n",
    "        sparse_model_train_list = sparse_list\n",
    "        # print(sparse_model_train_list)\n",
    "\n",
    "        if len(sparse_model_train_list) > 0:\n",
    "            sparse_model_train_data = pd.concat(sparse_model_train_list)\n",
    "            # print(sparse_model_train_data)\n",
    "            sparse_model_train_data = sparse_model_train_data.groupby(\n",
    "                sparse_model_train_data.index).sum()\n",
    "            # print(sparse_model_train_data)\n",
    "\n",
    "            sparse_model_forecast = sparse_model_train_data.max().values[0]\n",
    "            # adjust for holiday effect if current date is holiday effect date\n",
    "            # if sparse_model_entries['double_sell'].iloc[0]:\n",
    "            #     sparse_model_forecast = sparse_model_forecast * 2\n",
    "            # elif sparse_model_entries['triple_sell'].iloc[0]:\n",
    "            #     sparse_model_forecast = sparse_model_forecast * 3\n",
    "            # print('sparse_model_forecast:', sparse_model_forecast)\n",
    "        else:\n",
    "            sparse_model_forecast = 0\n",
    "    else:\n",
    "        sparse_model_forecast = 0\n",
    "        discarded_sparse_list = []\n",
    "\n",
    "    ret = dt_agent_prod_df[return_cols].iloc[:1]\n",
    "    norm_model_forecast = norm_model_entries['predicted_loading'].sum()\n",
    "    # ret['norm_model_forecast_back'] = norm_model_forecast\n",
    "    # ret['sparse_model_forecast_back'] = sparse_model_forecast\n",
    "    # if ret['holiday_effect'].iloc[0] == -1:\n",
    "    #     if (norm_model_forecast > 5) and (norm_model_forecast <= 15):\n",
    "    #         sparse_model_forecast = 0\n",
    "    #     else:\n",
    "    #         sparse_model_forecast = 0\n",
    "    #         norm_model_forecast = norm_model_forecast*0.7\n",
    "    ret['norm_model_forecast'] = norm_model_forecast\n",
    "    ret['sparse_model_forecast'] = sparse_model_forecast\n",
    "    ret['predicted_loading'] = norm_model_forecast + sparse_model_forecast\n",
    "    ret['known_shipment_norm'] = norm_model_entries['known_shipment'].sum()\n",
    "    ret['known_shipment_sparse'] = sparse_model_entries['known_shipment'].sum()\n",
    "    ret['non_sparse_count'] = len(norm_model_entries)\n",
    "    ret['sparse_count'] = len(dt_agent_prod_df) - ret['non_sparse_count']\n",
    "    ret['behavior_changed_sparse'] = str(discarded_sparse_list)\n",
    "    ret.index.name = 'discard'\n",
    "    # print(ret)\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "11px",
    "width": "160px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "262.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
